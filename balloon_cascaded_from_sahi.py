#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SAHIåˆ‡ç‰‡æ¨ç†ç»“æœçš„äºŒé˜¶æ®µåˆ†ç±»

åŠŸèƒ½ï¼š
1. è¯»å–SAHIåˆ‡ç‰‡æ¨ç†åçš„labelsï¼ˆYOLOæ ¼å¼ï¼‰
2. å¯¹æ¯ä¸ªæ£€æµ‹æ¡†ä½¿ç”¨MobileNetV2åˆ†ç±»å™¨è¿›è¡ŒäºŒæ¬¡éªŒè¯
3. è¿‡æ»¤è¯¯æ£€ï¼Œè¾“å‡ºç²¾ä¿®åçš„ç»“æœ
4. æ”¯æŒæ‰¹é‡å¤„ç†æ•´ä¸ªæ•°æ®é›†
"""

import os
import sys
import cv2
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import argparse
from tqdm import tqdm
from PIL import Image
import torchvision.transforms as transforms

# å¯¼å…¥åˆ†ç±»å™¨æ¨¡å‹
sys.path.insert(0, str(Path(__file__).parent))
from balloon_cascaded_detection import MobileNetClassifier, SimpleMLP


class SAHIResultRefiner:
    """SAHIç»“æœç²¾ä¿®å™¨ï¼ˆä½¿ç”¨äºŒé˜¶æ®µåˆ†ç±»å™¨ï¼‰"""
    
    def __init__(self, classifier_path: str, classifier_type: str = "mobilenet",
                 input_size: int = 112, num_classes: int = 2,
                 threshold: float = 0.5, device: str = "cuda:0"):
        """
        åˆå§‹åŒ–ç²¾ä¿®å™¨
        
        Args:
            classifier_path: åˆ†ç±»å™¨æƒé‡è·¯å¾„
            classifier_type: åˆ†ç±»å™¨ç±»å‹ ('mlp' æˆ– 'mobilenet')
            input_size: åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸
            num_classes: ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
            threshold: åˆ†ç±»é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§†ä¸ºèƒŒæ™¯ï¼‰
            device: è®¾å¤‡
        """
        self.device = device
        self.threshold = threshold
        self.input_size = input_size
        
        # åŠ è½½åˆ†ç±»å™¨
        if classifier_type == "mlp":
            self.classifier = SimpleMLP(input_size, num_classes)
        else:
            self.classifier = MobileNetClassifier(num_classes)
        
        self.classifier.load_state_dict(torch.load(classifier_path, map_location=device))
        self.classifier.to(device)
        self.classifier.eval()
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((input_size, input_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        print(f"âœ… åŠ è½½SAHIç»“æœç²¾ä¿®å™¨")
        print(f"   åˆ†ç±»å™¨: {classifier_path}")
        print(f"   ç±»å‹: {classifier_type}")
        print(f"   é˜ˆå€¼: {threshold}")
    
    def parse_yolo_label(self, label_path: str, img_w: int, img_h: int) -> List[Dict]:
        """
        è§£æYOLOæ ¼å¼æ ‡ç­¾æ–‡ä»¶
        
        Returns:
            [{'cls': int, 'conf': float, 'box': [x1, y1, x2, y2]}, ...]
        """
        detections = []
        
        if not os.path.exists(label_path):
            return detections
        
        with open(label_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    cls = int(parts[0])
                    x_center = float(parts[1]) * img_w
                    y_center = float(parts[2]) * img_h
                    w = float(parts[3]) * img_w
                    h = float(parts[4]) * img_h
                    
                    # å¦‚æœæœ‰ç½®ä¿¡åº¦
                    conf = float(parts[5]) if len(parts) >= 6 else 1.0
                    
                    x1 = x_center - w / 2
                    y1 = y_center - h / 2
                    x2 = x_center + w / 2
                    y2 = y_center + h / 2
                    
                    detections.append({
                        'cls': cls,
                        'conf': conf,
                        'box': [x1, y1, x2, y2]
                    })
        
        return detections
    
    def refine_detections(self, image_path: str, label_path: str) -> List[Dict]:
        """
        ç²¾ä¿®SAHIæ£€æµ‹ç»“æœ
        
        Args:
            image_path: åŸå§‹å›¾åƒè·¯å¾„
            label_path: SAHIæ¨ç†ç»“æœlabelè·¯å¾„ï¼ˆYOLOæ ¼å¼ï¼‰
        
        Returns:
            ç²¾ä¿®åçš„æ£€æµ‹ç»“æœ [{'cls': int, 'conf': float, 'box': [x1,y1,x2,y2], 
                              'stage1_cls': int, 'stage1_conf': float}, ...]
        """
        # è¯»å–å›¾åƒ
        img = cv2.imread(image_path)
        img_h, img_w = img.shape[:2]
        
        # è§£æSAHIç»“æœ
        stage1_detections = self.parse_yolo_label(label_path, img_w, img_h)
        
        refined_detections = []
        
        for det in stage1_detections:
            x1, y1, x2, y2 = det['box']
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            
            # è¾¹ç•Œæ£€æŸ¥
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(img_w, x2)
            y2 = min(img_h, y2)
            
            if x2 <= x1 or y2 <= y1:
                continue
            
            # è£å‰ªå€™é€‰åŒºåŸŸ
            crop = img[y1:y2, x1:x2]
            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
            
            # åˆ†ç±»å™¨æ¨ç†
            crop_tensor = self.transform(crop_pil).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                output = self.classifier(crop_tensor)
                probs = F.softmax(output, dim=1)
                stage2_conf, stage2_cls = probs.max(1)
                stage2_conf = stage2_conf.item()
                stage2_cls = stage2_cls.item()
            
            # è¿‡æ»¤ï¼šå¦‚æœåˆ†ç±»å™¨åˆ¤æ–­ä¸ºèƒŒæ™¯ï¼ˆcls=0ï¼‰æˆ–ç½®ä¿¡åº¦ä½ï¼Œåˆ™ä¸¢å¼ƒ
            if stage2_cls == 0 or stage2_conf < self.threshold:
                continue
            
            # è½¬æ¢ç±»åˆ«ï¼ˆåˆ†ç±»å™¨çš„ç±»åˆ«ä»1å¼€å§‹ï¼Œéœ€è¦å‡1ï¼‰
            final_cls = stage2_cls - 1
            
            refined_detections.append({
                'cls': final_cls,
                'conf': stage2_conf,
                'box': [float(x1), float(y1), float(x2), float(y2)],
                'stage1_cls': det['cls'],
                'stage1_conf': det['conf']
            })
        
        return refined_detections
    
    def save_yolo_label(self, detections: List[Dict], save_path: str, img_w: int, img_h: int):
        """ä¿å­˜ä¸ºYOLOæ ¼å¼æ ‡ç­¾"""
        with open(save_path, 'w') as f:
            for det in detections:
                x1, y1, x2, y2 = det['box']
                x_center = (x1 + x2) / 2 / img_w
                y_center = (y1 + y2) / 2 / img_h
                w = (x2 - x1) / img_w
                h = (y2 - y1) / img_h
                
                f.write(f"{det['cls']} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\n")
    
    def visualize_comparison(self, image_path: str, stage1_dets: List[Dict], 
                            stage2_dets: List[Dict], save_path: str, class_names: List[str]):
        """å¯è§†åŒ–å¯¹æ¯”ï¼šSAHIç»“æœ vs äºŒé˜¶æ®µç²¾ä¿®ç»“æœ"""
        import matplotlib.pyplot as plt
        import matplotlib.patches as patches
        
        img = cv2.imread(image_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        fig, axes = plt.subplots(1, 2, figsize=(20, 10))
        
        # SAHIç»“æœ
        axes[0].imshow(img_rgb)
        axes[0].set_title(f"SAHIç»“æœ ({len(stage1_dets)} æ£€æµ‹)", fontsize=16)
        axes[0].axis('off')
        
        for det in stage1_dets:
            x1, y1, x2, y2 = det['box']
            w, h = x2 - x1, y2 - y1
            rect = patches.Rectangle((x1, y1), w, h, linewidth=2, 
                                     edgecolor='red', facecolor='none')
            axes[0].add_patch(rect)
            
            cls_name = class_names[det['cls']] if det['cls'] < len(class_names) else f"cls{det['cls']}"
            axes[0].text(x1, y1-5, f"{cls_name} {det['conf']:.2f}",
                        color='red', fontsize=10, weight='bold',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
        
        # äºŒé˜¶æ®µç²¾ä¿®ç»“æœ
        axes[1].imshow(img_rgb)
        axes[1].set_title(f"äºŒé˜¶æ®µç²¾ä¿® ({len(stage2_dets)} æ£€æµ‹)", fontsize=16)
        axes[1].axis('off')
        
        for det in stage2_dets:
            x1, y1, x2, y2 = det['box']
            w, h = x2 - x1, y2 - y1
            rect = patches.Rectangle((x1, y1), w, h, linewidth=2, 
                                     edgecolor='green', facecolor='none')
            axes[1].add_patch(rect)
            
            cls_name = class_names[det['cls']] if det['cls'] < len(class_names) else f"cls{det['cls']}"
            axes[1].text(x1, y1-5, f"{cls_name} {det['conf']:.2f}",
                        color='green', fontsize=10, weight='bold',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()


def refine_dataset(sahi_results_dir: str, images_dir: str, classifier_path: str,
                   save_dir: str, classifier_type: str = "mobilenet",
                   input_size: int = 112, num_classes: int = 2,
                   threshold: float = 0.5, device: str = "cuda:0",
                   visualize: bool = True, class_names: List[str] = None):
    """
    æ‰¹é‡ç²¾ä¿®SAHIç»“æœ
    
    Args:
        sahi_results_dir: SAHIæ¨ç†ç»“æœç›®å½•ï¼ˆåŒ…å«labelså­ç›®å½•ï¼‰
        images_dir: åŸå§‹å›¾åƒç›®å½•
        classifier_path: åˆ†ç±»å™¨æƒé‡è·¯å¾„
        save_dir: ä¿å­˜ç›®å½•
        å…¶ä»–å‚æ•°åŒSAHIResultRefiner
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_path = Path(save_dir)
    labels_save_dir = save_path / "labels"
    vis_save_dir = save_path / "visualizations"
    labels_save_dir.mkdir(parents=True, exist_ok=True)
    if visualize:
        vis_save_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–ç²¾ä¿®å™¨
    refiner = SAHIResultRefiner(
        classifier_path, classifier_type, input_size,
        num_classes, threshold, device
    )
    
    # è·å–æ‰€æœ‰labelæ–‡ä»¶
    sahi_labels_dir = Path(sahi_results_dir) / "labels"
    label_files = list(sahi_labels_dir.glob("*.txt"))
    
    print(f"\nğŸ“‚ å¤„ç†SAHIç»“æœ...")
    print(f"   SAHIç»“æœ: {sahi_results_dir}")
    print(f"   å›¾åƒç›®å½•: {images_dir}")
    print(f"   ä¿å­˜ç›®å½•: {save_dir}")
    print(f"   æ ‡ç­¾æ–‡ä»¶æ•°: {len(label_files)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_stage1 = 0
    total_stage2 = 0
    filtered_count = 0
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for label_file in tqdm(label_files, desc="ç²¾ä¿®SAHIç»“æœ"):
        # æ‰¾åˆ°å¯¹åº”çš„å›¾åƒ
        img_name = label_file.stem
        img_path = None
        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            candidate = Path(images_dir) / f"{img_name}{ext}"
            if candidate.exists():
                img_path = str(candidate)
                break
        
        if img_path is None:
            print(f"âš ï¸  æ‰¾ä¸åˆ°å›¾åƒ: {img_name}")
            continue
        
        # ç²¾ä¿®æ£€æµ‹ç»“æœ
        stage1_dets = refiner.parse_yolo_label(str(label_file), 
                                               cv2.imread(img_path).shape[1],
                                               cv2.imread(img_path).shape[0])
        stage2_dets = refiner.refine_detections(img_path, str(label_file))
        
        # ç»Ÿè®¡
        total_stage1 += len(stage1_dets)
        total_stage2 += len(stage2_dets)
        filtered_count += (len(stage1_dets) - len(stage2_dets))
        
        # ä¿å­˜ç²¾ä¿®åçš„labels
        img = cv2.imread(img_path)
        img_h, img_w = img.shape[:2]
        save_label_path = labels_save_dir / label_file.name
        refiner.save_yolo_label(stage2_dets, str(save_label_path), img_w, img_h)
        
        # å¯è§†åŒ–
        if visualize:
            vis_path = vis_save_dir / f"{img_name}_comparison.jpg"
            if class_names is None:
                class_names = [f"cls{i}" for i in range(num_classes)]
            refiner.visualize_comparison(img_path, stage1_dets, stage2_dets,
                                        str(vis_path), class_names)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… ç²¾ä¿®å®Œæˆ!")
    print(f"   å¤„ç†å›¾åƒæ•°: {len(label_files)}")
    print(f"   SAHIæ£€æµ‹æ€»æ•°: {total_stage1}")
    print(f"   ç²¾ä¿®åæ£€æµ‹æ€»æ•°: {total_stage2}")
    print(f"   è¿‡æ»¤æ£€æµ‹æ•°: {filtered_count} ({100*filtered_count/max(total_stage1,1):.1f}%)")
    print(f"   ä¿ç•™ç‡: {100*total_stage2/max(total_stage1,1):.1f}%")
    print(f"\nğŸ“ ç»“æœä¿å­˜è‡³:")
    print(f"   Labels: {labels_save_dir}")
    if visualize:
        print(f"   å¯è§†åŒ–: {vis_save_dir}")


def main():
    parser = argparse.ArgumentParser(description="SAHIç»“æœçš„äºŒé˜¶æ®µç²¾ä¿®")
    
    # è¾“å…¥è¾“å‡º
    parser.add_argument('--sahi-results', type=str, required=True,
                       help='SAHIæ¨ç†ç»“æœç›®å½•ï¼ˆåŒ…å«labelså­ç›®å½•ï¼‰')
    parser.add_argument('--images', type=str, required=True,
                       help='åŸå§‹å›¾åƒç›®å½•')
    parser.add_argument('--classifier', type=str, required=True,
                       help='åˆ†ç±»å™¨æƒé‡è·¯å¾„')
    parser.add_argument('--save-dir', type=str, required=True,
                       help='ä¿å­˜ç›®å½•')
    
    # åˆ†ç±»å™¨å‚æ•°
    parser.add_argument('--model-type', type=str, default='mobilenet',
                       choices=['mlp', 'mobilenet'], help='åˆ†ç±»å™¨ç±»å‹')
    parser.add_argument('--input-size', type=int, default=112,
                       help='åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸')
    parser.add_argument('--num-classes', type=int, default=2,
                       help='ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰')
    parser.add_argument('--threshold', type=float, default=0.5,
                       help='åˆ†ç±»é˜ˆå€¼')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    parser.add_argument('--no-visualize', action='store_true',
                       help='ä¸ç”Ÿæˆå¯è§†åŒ–å›¾åƒ')
    parser.add_argument('--class-names', type=str, nargs='+',
                       help='ç±»åˆ«åç§°åˆ—è¡¨')
    
    args = parser.parse_args()
    
    # è¿è¡Œç²¾ä¿®
    refine_dataset(
        sahi_results_dir=args.sahi_results,
        images_dir=args.images,
        classifier_path=args.classifier,
        save_dir=args.save_dir,
        classifier_type=args.model_type,
        input_size=args.input_size,
        num_classes=args.num_classes,
        threshold=args.threshold,
        device=args.device,
        visualize=not args.no_visualize,
        class_names=args.class_names
    )


if __name__ == '__main__':
    main()

