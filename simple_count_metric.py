#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªå®šä¹‰æ£€æµ‹æ•°é‡å¯¹æ¯”Metric - ç®€åŒ–ç‰ˆæœ¬
å®ç°å…¬å¼: 1 - |pred_count - true_count| / true_count

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-01-29
"""

import pickle
import numpy as np
from pathlib import Path


class SimpleCountMetric:
    """ç®€åŒ–çš„æ£€æµ‹æ•°é‡å¯¹æ¯”Metric"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®"""
        self.pred_counts = []
        self.true_counts = []
        self.image_names = []
    
    def update(self, pred_count: int, true_count: int, image_name: str = ""):
        """æ›´æ–°metric"""
        self.pred_counts.append(pred_count)
        self.true_counts.append(true_count)
        self.image_names.append(image_name)
    
    def compute_metric(self, pred_count: int, true_count: int) -> float:
        """è®¡ç®—å•ä¸ªmetricå€¼"""
        if true_count > 0:
            return 1 - abs(pred_count - true_count) / true_count
        else:
            return 1.0 if pred_count == 0 else float('-inf')
    
    def get_results(self):
        """è·å–ç»“æœ"""
        if not self.pred_counts:
            return {"error": "æ²¡æœ‰æ•°æ®"}
        
        # è®¡ç®—æ‰€æœ‰metricå€¼
        metric_values = []
        for pred, true in zip(self.pred_counts, self.true_counts):
            metric_values.append(self.compute_metric(pred, true))
        
        # è¿‡æ»¤æœ‰æ•ˆå€¼
        valid_values = [v for v in metric_values if v != float('-inf')]
        
        return {
            "total_images": len(self.pred_counts),
            "valid_images": len(valid_values),
            "total_pred_boxes": sum(self.pred_counts),
            "total_true_boxes": sum(self.true_counts),
            "mean_metric": np.mean(valid_values) if valid_values else 0.0,
            "min_metric": np.min(valid_values) if valid_values else 0.0,
            "max_metric": np.max(valid_values) if valid_values else 0.0,
            "perfect_matches": sum(1 for v in valid_values if v == 1.0),
            "good_predictions": sum(1 for v in valid_values if v >= 0.8),
            "poor_predictions": sum(1 for v in valid_values if v < 0.5),
            "detailed_results": [
                {
                    "image_name": name,
                    "pred_count": pred,
                    "true_count": true,
                    "metric_value": metric
                }
                for name, pred, true, metric in zip(
                    self.image_names, self.pred_counts, self.true_counts, metric_values
                )
            ]
        }


def load_ground_truth(labels_dir: str) -> dict:
    """åŠ è½½çœŸå®æ ‡ç­¾"""
    labels_dir = Path(labels_dir)
    ground_truth = {}
    
    for label_file in labels_dir.glob("*.txt"):
        image_name = label_file.stem + ".jpg"
        with open(label_file, 'r') as f:
            lines = f.readlines()
        true_count = len([line for line in lines if line.strip()])
        ground_truth[image_name] = true_count
    
    return ground_truth


def evaluate_from_pkl(pkl_path: str, labels_dir: str):
    """ä»PKLæ–‡ä»¶è¯„ä¼°"""
    print(f"ğŸ“Š ä»PKLæ–‡ä»¶è¯„ä¼°: {pkl_path}")
    
    # åŠ è½½PKLæ–‡ä»¶
    with open(pkl_path, 'rb') as f:
        pkl_data = pickle.load(f)
    
    # åŠ è½½çœŸå®æ ‡ç­¾
    ground_truth = load_ground_truth(labels_dir)
    
    # åˆ›å»ºmetric
    metric = SimpleCountMetric()
    
    # å¤„ç†æ•°æ®
    if "results" in pkl_data:
        results = pkl_data["results"]
    else:
        results = [pkl_data]
    
    for result in results:
        image_name = result["image_name"]
        pred_count = result["num_detections"]
        true_count = ground_truth.get(image_name, 0)
        metric.update(pred_count, true_count, image_name)
    
    return metric.get_results()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ è‡ªå®šä¹‰æ£€æµ‹æ•°é‡å¯¹æ¯”Metric")
    print("å…¬å¼: 1 - |pred_count - true_count| / true_count")
    print("="*60)
    
    # æµ‹è¯•å…¬å¼
    print("ğŸ§ª å…¬å¼æµ‹è¯•:")
    test_cases = [
        (5, 5, "å®Œç¾åŒ¹é…"),
        (4, 5, "å°‘é¢„æµ‹1ä¸ª"),
        (6, 5, "å¤šé¢„æµ‹1ä¸ª"),
        (0, 5, "å®Œå…¨æ¼æ£€"),
        (10, 5, "å¤šé¢„æµ‹5ä¸ª"),
        (15, 5, "å¤šé¢„æµ‹10ä¸ª"),
    ]
    
    metric = SimpleCountMetric()
    for pred, true, desc in test_cases:
        metric_value = metric.compute_metric(pred, true)
        print(f"   é¢„æµ‹={pred:2d}, çœŸå®={true:2d}, Metric={metric_value:6.3f}, {desc}")
    
    print("\nğŸˆ Balloonæ•°æ®é›†è¯„ä¼°:")
    
    # è¯„ä¼°å‚æ•°
    pkl_path = "/public/home/baichen/download/dcu_yolo/ultralytics/runs/inference_pkl/D1_yolo11m_inference_pkl_results/all_inference_results.pkl"
    labels_dir = "/public/home/baichen/download/dcu_yolo/ultralytics/data/D1_type3/yolo_format/labels/val"
    
    if Path(pkl_path).exists():
        results = evaluate_from_pkl(pkl_path, labels_dir)
        
        print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"   æ€»å›¾åƒæ•°: {results['total_images']}")
        print(f"   æœ‰æ•ˆå›¾åƒæ•°: {results['valid_images']}")
        print(f"   æ€»é¢„æµ‹æ¡†æ•°: {results['total_pred_boxes']}")
        print(f"   æ€»çœŸå®æ¡†æ•°: {results['total_true_boxes']}")
        print(f"   å¹³å‡Metricå€¼: {results['mean_metric']:.4f}")
        print(f"   MetricèŒƒå›´: [{results['min_metric']:.4f}, {results['max_metric']:.4f}]")
        print(f"   å®Œç¾åŒ¹é…: {results['perfect_matches']} å¼ ")
        print(f"   è‰¯å¥½é¢„æµ‹: {results['good_predictions']} å¼ ")
        print(f"   è¾ƒå·®é¢„æµ‹: {results['poor_predictions']} å¼ ")
        
        print(f"\nğŸ“¸ å‰5ä¸ªè¯¦ç»†ç»“æœ:")
        for i, detail in enumerate(results['detailed_results'][:5]):
            print(f"   {i+1}. {detail['image_name']}: "
                  f"é¢„æµ‹={detail['pred_count']}, çœŸå®={detail['true_count']}, "
                  f"Metric={detail['metric_value']:.4f}")
        
        # åˆ†æç»“æœ
        print(f"\nğŸ“Š ç»“æœåˆ†æ:")
        avg_pred = results['total_pred_boxes'] / max(results['total_images'],1)
        avg_true = results['total_true_boxes'] / max(results['total_images'],1)
        print(f"   å¹³å‡é¢„æµ‹æ¡†æ•°: {avg_pred:.2f}")
        print(f"   å¹³å‡çœŸå®æ¡†æ•°: {avg_true:.2f}")
        # print(f"   é¢„æµ‹/çœŸå®æ¯”ä¾‹: {avg_pred/avg_true:.2f}")
        
        if results['mean_metric'] < 0:
            print(f"   âš ï¸  æ¨¡å‹å­˜åœ¨ä¸¥é‡è¿‡æ£€æµ‹é—®é¢˜")
        elif results['mean_metric'] < 0.5:
            print(f"   âš ï¸  æ¨¡å‹æ£€æµ‹æ•°é‡å‡†ç¡®æ€§è¾ƒå·®")
        elif results['mean_metric'] < 0.8:
            print(f"   âœ… æ¨¡å‹æ£€æµ‹æ•°é‡å‡†ç¡®æ€§ä¸€èˆ¬")
        else:
            print(f"   ğŸ‰ æ¨¡å‹æ£€æµ‹æ•°é‡å‡†ç¡®æ€§å¾ˆå¥½")
    
    else:
        print(f"âŒ PKLæ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
    
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()
