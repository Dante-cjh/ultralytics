#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
DOTA æ•°æ®åˆ‡ç‰‡ + è®­ç»ƒé›†æˆè„šæœ¬
å°†æ•°æ®åˆ‡ç‰‡å’Œæ¨¡å‹è®­ç»ƒé›†æˆåˆ°ä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµä¸­
"""

import argparse
import sys
import time
from pathlib import Path
from typing import Tuple, Optional

from ultralytics import YOLO
from ultralytics.data.split_dota import split_trainval, split_test
from ultralytics.utils import LOGGER


class DotaTrainingPipeline:
    """DOTA æ•°æ®åˆ‡ç‰‡å’Œè®­ç»ƒæµæ°´çº¿"""
    
    def __init__(
        self,
        data_root: str,
        slice_dir: str,
        model_name: str = "yolo11l-obb.pt",
        project_name: str = "dota_yolo11l_slice"
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒæµæ°´çº¿
        
        Args:
            data_root (str): åŸå§‹ DOTA æ•°æ®æ ¹ç›®å½•
            slice_dir (str): åˆ‡ç‰‡åæ•°æ®ä¿å­˜ç›®å½•
            model_name (str): æ¨¡å‹åç§°æˆ–è·¯å¾„
            project_name (str): è®­ç»ƒé¡¹ç›®åç§°
        """
        self.data_root = Path(data_root)
        self.slice_dir = Path(slice_dir)
        self.model_name = model_name
        self.project_name = project_name
        
        # éªŒè¯è·¯å¾„
        if not self.data_root.exists():
            raise ValueError(f"æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {self.data_root}")
    
    def check_data_structure(self) -> bool:
        """æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„"""
        LOGGER.info("ğŸ” æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„...")
        
        required_dirs = [
            "images/train",
            "images/val",
            "labels/train", 
            "labels/val"
        ]
        
        missing_dirs = []
        for dir_path in required_dirs:
            full_path = self.data_root / dir_path
            if not full_path.exists():
                missing_dirs.append(str(full_path))
        
        if missing_dirs:
            LOGGER.error("âŒ æ•°æ®ç›®å½•ç»“æ„ä¸å®Œæ•´")
            for missing_dir in missing_dirs:
                LOGGER.error(f"  ç¼ºå°‘: {missing_dir}")
            return False
        
        LOGGER.info("âœ… æ•°æ®ç›®å½•ç»“æ„æ£€æŸ¥é€šè¿‡")
        return True
    
    def get_dataset_stats(self, data_path: Path) -> dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for split in ["train", "val"]:
            img_dir = data_path / "images" / split
            lbl_dir = data_path / "labels" / split
            
            img_count = len(list(img_dir.glob("*.*"))) if img_dir.exists() else 0
            lbl_count = len(list(lbl_dir.glob("*.txt"))) if lbl_dir.exists() else 0
            
            stats[split] = {"images": img_count, "labels": lbl_count}
        
        return stats
    
    def slice_data(
        self,
        crop_size: int = 1024,
        gap: int = 200,
        rates: Tuple[float, ...] = (1.0,),
        force_slice: bool = False
    ) -> bool:
        """
        æ‰§è¡Œæ•°æ®åˆ‡ç‰‡
        
        Args:
            crop_size (int): åˆ‡ç‰‡çª—å£å¤§å°
            gap (int): çª—å£é‡å å¤§å°
            rates (Tuple[float, ...]): å¤šå°ºåº¦ç¼©æ”¾æ¯”ä¾‹
            force_slice (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ‡ç‰‡
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        LOGGER.info("ğŸ“¸ å¼€å§‹æ•°æ®åˆ‡ç‰‡...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ‡ç‰‡
        if self.slice_dir.exists() and not force_slice:
            slice_stats = self.get_dataset_stats(self.slice_dir)
            if slice_stats["train"]["images"] > 0:
                LOGGER.info(f"âœ… å‘ç°å·²åˆ‡ç‰‡çš„æ•°æ®: {self.slice_dir}")
                LOGGER.info(f"   è®­ç»ƒé›†: {slice_stats['train']['images']} å›¾åƒ")
                LOGGER.info(f"   éªŒè¯é›†: {slice_stats['val']['images']} å›¾åƒ")
                LOGGER.info("   ä½¿ç”¨ --force-slice å¼ºåˆ¶é‡æ–°åˆ‡ç‰‡")
                return True
        
        # è·å–åŸå§‹æ•°æ®ç»Ÿè®¡
        orig_stats = self.get_dataset_stats(self.data_root)
        LOGGER.info(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
        for split, stats in orig_stats.items():
            LOGGER.info(f"  {split}: {stats['images']} å›¾åƒ, {stats['labels']} æ ‡ç­¾")
        
        LOGGER.info(f"åˆ‡ç‰‡å‚æ•°:")
        LOGGER.info(f"  çª—å£å¤§å°: {crop_size}x{crop_size}")
        LOGGER.info(f"  é‡å å¤§å°: {gap}")
        LOGGER.info(f"  ç¼©æ”¾æ¯”ä¾‹: {rates}")
        
        try:
            start_time = time.time()
            
            # æ‰§è¡Œåˆ‡ç‰‡
            split_trainval(
                data_root=str(self.data_root),
                save_dir=str(self.slice_dir),
                crop_size=crop_size,
                gap=gap,
                rates=rates
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # è·å–åˆ‡ç‰‡åç»Ÿè®¡
            slice_stats = self.get_dataset_stats(self.slice_dir)
            LOGGER.info(f"âœ… æ•°æ®åˆ‡ç‰‡å®Œæˆ (è€—æ—¶: {duration:.1f}s)")
            LOGGER.info(f"åˆ‡ç‰‡åæ•°æ®ç»Ÿè®¡:")
            for split, stats in slice_stats.items():
                LOGGER.info(f"  {split}: {stats['images']} å›¾åƒ, {stats['labels']} æ ‡ç­¾")
            
            return True
            
        except Exception as e:
            LOGGER.error(f"âŒ æ•°æ®åˆ‡ç‰‡å¤±è´¥: {e}")
            return False
    

    
    def train_model(
        self,
        epochs: int = 100,
        imgsz: int = 1024,
        batch: int = 16,
        device: int = 0,
        resume: bool = False
    ) -> Optional[str]:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            epochs (int): è®­ç»ƒè½®æ•°
            imgsz (int): è¾“å…¥å›¾åƒå°ºå¯¸
            batch (int): æ‰¹æ¬¡å¤§å°
            device (int): GPU è®¾å¤‡ç¼–å·
            resume (bool): æ˜¯å¦æ¢å¤è®­ç»ƒ
        
        Returns:
            str: æœ€ä½³æ¨¡å‹è·¯å¾„
        """
        LOGGER.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        
        # ä½¿ç”¨é¢„å®šä¹‰çš„æ•°æ®é›†é…ç½®æ–‡ä»¶
        dataset_yaml = "dota_slice.yaml"
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = YOLO(self.model_name)
        LOGGER.info(f"æ¨¡å‹: {self.model_name}")
        LOGGER.info(f"æ•°æ®: {dataset_yaml}")
        LOGGER.info(f"è®­ç»ƒå‚æ•°: epochs={epochs}, imgsz={imgsz}, batch={batch}")
        
        try:
            # å¼€å§‹è®­ç»ƒ
            results = model.train(
                data=str(dataset_yaml),

                # === åŸºç¡€è®­ç»ƒå‚æ•° ===
                epochs=epochs,               # è®­ç»ƒè½®æ•°
                imgsz=imgsz,               # å›¾åƒå°ºå¯¸ (æå‡å°ç›®æ ‡æ£€æµ‹)
                batch=batch,                  # æ‰¹å¤§å° (é€‚é…4090/24Gåœ¨1280è¾“å…¥)
                device=device,                  # GPUè®¾å¤‡
                
                # === é¡¹ç›®ç®¡ç† ===
                project="runs/obb",
                name=f"{self.project_name}",
                resume=resume,
                exist_ok=True,            # å…è®¸è¦†ç›–ç°æœ‰å®éªŒ

                # === æ—©åœå’Œä¿å­˜ ===
                patience=30,              # æ—©åœè€å¿ƒå€¼
                save_period=20,           # æ¯20è½®ä¿å­˜ä¸€æ¬¡

                # === è®­ç»ƒä¼˜åŒ– ===
                amp=True,                # å¯ç”¨AMP (OBBä»»åŠ¡å…¼å®¹æ€§é—®é¢˜)
                cache=False,             # ä¸ç¼“å­˜å›¾åƒ (æ•°æ®é›†è¾ƒå°)
                rect=False,              # ä¸ä½¿ç”¨çŸ©å½¢è®­ç»ƒ
                cos_lr=True,             # ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ (æ³›åŒ–æ›´ç¨³)
                lr0=0.01,                # åˆå§‹å­¦ä¹ ç‡
                lrf=0.01,                # æœ€ç»ˆå­¦ä¹ ç‡å› å­

                # === å…¶ä»–è®¾ç½® ===
                workers=4,               # å•çº¿ç¨‹æ•°æ®åŠ è½½ï¼ˆé¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼‰
                verbose=True,            # è¯¦ç»†è¾“å‡º
                seed=42,                 # éšæœºç§å­ï¼Œä¿è¯å¯é‡ç°æ€§
                deterministic=True,      # ç¡®å®šæ€§è®­ç»ƒ
                single_cls=False,        # å¤šç±»åˆ«è®­ç»ƒ (è™½ç„¶åªæœ‰1ç±»)
                plots=True,              # ç”Ÿæˆè®­ç»ƒå›¾è¡¨

                # === æ•°æ®å¢å¼ºè®¾ç½® ===
                degrees=180,             # æ—‹è½¬ç­‰å˜æ€§ (å¯¹é¥æ„Ÿ/OBBå¾ˆå…³é”®)
                flipud=0.5,              # ç«–ç›´ç¿»è½¬ (ä¿¯è§†å›¾æ”¶ç›Šæ˜æ˜¾)
                fliplr=0.5,              # æ°´å¹³ç¿»è½¬
                mosaic=1.0,              # Mosaicå¢å¼º (å¯¹å°ç›®æ ‡å‹å¥½)
                close_mosaic=10,         # å…³é—­Mosaicçš„epoch
                mixup=0.1,               # è½»é‡æ··åˆå¢å¼º
                erasing=0.2,             # éšæœºæ“¦é™¤ (é¿å…æŠŠå°ç›®æ ‡æŠ¹æ‰)
                translate=0.2,           # å¹³ç§»å¢å¼º
                
                # === æŸå¤±å‡½æ•°æƒé‡ ===
                box=9.0,                 # é€‚åº¦æé«˜å®šä½æŸå¤±æƒé‡

                # === éªŒè¯è®¾ç½® ===
                val=True,                # è®­ç»ƒæ—¶è¿›è¡ŒéªŒè¯
                split='val',             # éªŒè¯é›†åˆ†å‰²
                save_json=False,         # ä¸ä¿å­˜JSONç»“æœ (å•ç±»åˆ«ä¸éœ€è¦)
                save_hybrid=False,       # ä¸ä¿å­˜hybridæ ‡ç­¾
            )
            
            LOGGER.info("âœ… è®­ç»ƒå®Œæˆï¼")
            
            # è¿”å›æœ€ä½³æ¨¡å‹è·¯å¾„
            best_model = Path("runs/obb") / self.project_name / "weights" / "best.pt"
            if best_model.exists():
                LOGGER.info(f"æœ€ä½³æ¨¡å‹: {best_model}")
                return str(best_model)
            else:
                LOGGER.warning(f"æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {best_model}")
                # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ¨¡å‹æ–‡ä»¶
                weights_dir = Path("runs/obb") / self.project_name / "weights"
                if weights_dir.exists():
                    model_files = list(weights_dir.glob("*.pt"))
                    if model_files:
                        latest_model = max(model_files, key=lambda x: x.stat().st_mtime)
                        LOGGER.info(f"æ‰¾åˆ°æœ€æ–°æ¨¡å‹: {latest_model}")
                        return str(latest_model)
            
            return None
            
        except Exception as e:
            LOGGER.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            return None
    
    def run_complete_pipeline(
        self,
        # åˆ‡ç‰‡å‚æ•°
        crop_size: int = 1024,
        gap: int = 200,
        rates: Tuple[float, ...] = (1.0,),
        force_slice: bool = False,
        # è®­ç»ƒå‚æ•°
        epochs: int = 100,
        imgsz: int = 1024,
        batch: int = 16,
        device: int = 0,
        resume: bool = False
    ) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        LOGGER.info("ğŸ¯ å¼€å§‹ DOTA å®Œæ•´è®­ç»ƒæµæ°´çº¿")
        LOGGER.info(f"é¡¹ç›®åç§°: {self.project_name}")
        LOGGER.info(f"åŸå§‹æ•°æ®: {self.data_root}")
        LOGGER.info(f"åˆ‡ç‰‡æ•°æ®: {self.slice_dir}")
        
        # 1. æ£€æŸ¥æ•°æ®ç»“æ„
        if not self.check_data_structure():
            return False
        
        # 2. æ•°æ®åˆ‡ç‰‡
        if not self.slice_data(crop_size, gap, rates, force_slice):
            return False
        
        # 3. æ¨¡å‹è®­ç»ƒ
        best_model = self.train_model(epochs, imgsz, batch, device, resume)
        if best_model is None:
            return False
        
        LOGGER.info("ğŸ‰ å®Œæ•´è®­ç»ƒæµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
        LOGGER.info(f"æœ€ä½³æ¨¡å‹: {best_model}")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DOTA æ•°æ®åˆ‡ç‰‡å’Œè®­ç»ƒé›†æˆè„šæœ¬")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data-root", type=str, required=True, help="åŸå§‹ DOTA æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--slice-dir", type=str, required=True, help="åˆ‡ç‰‡åæ•°æ®ä¿å­˜ç›®å½•")
    parser.add_argument("--project-name", type=str, default="dota_yolo11l_slice", help="è®­ç»ƒé¡¹ç›®åç§°")
    
    # åˆ‡ç‰‡å‚æ•°
    parser.add_argument("--crop-size", type=int, default=1024, help="åˆ‡ç‰‡çª—å£å¤§å°")
    parser.add_argument("--gap", type=int, default=200, help="çª—å£é‡å å¤§å°")
    parser.add_argument("--rates", nargs="+", type=float, default=[1.0], help="å¤šå°ºåº¦ç¼©æ”¾æ¯”ä¾‹")
    parser.add_argument("--force-slice", action="store_true", help="å¼ºåˆ¶é‡æ–°åˆ‡ç‰‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--model", type=str, default="yolo11l-obb.pt", help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=200, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--imgsz", type=int, default=1024, help="è¾“å…¥å›¾åƒå°ºå¯¸")
    parser.add_argument("--batch", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--device", type=int, default=5, help="GPU è®¾å¤‡ç¼–å·")
    parser.add_argument("--resume", action="store_true", help="æ¢å¤è®­ç»ƒ")
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument("--slice-only", action="store_true", help="ä»…æ‰§è¡Œæ•°æ®åˆ‡ç‰‡")
    parser.add_argument("--train-only", action="store_true", help="ä»…æ‰§è¡Œæ¨¡å‹è®­ç»ƒ")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºè®­ç»ƒæµæ°´çº¿
        pipeline = DotaTrainingPipeline(
            data_root=args.data_root,
            slice_dir=args.slice_dir,
            model_name=args.model,
            project_name=args.project_name
        )
        
        if args.slice_only:
            # ä»…æ‰§è¡Œåˆ‡ç‰‡
            success = pipeline.slice_data(
                crop_size=args.crop_size,
                gap=args.gap,
                rates=tuple(args.rates),
                force_slice=args.force_slice
            )
        elif args.train_only:
            # ä»…æ‰§è¡Œè®­ç»ƒ
            best_model = pipeline.train_model(
                epochs=args.epochs,
                imgsz=args.imgsz,
                batch=args.batch,
                device=args.device,
                resume=args.resume
            )
            success = best_model is not None
        else:
            # æ‰§è¡Œå®Œæ•´æµæ°´çº¿
            success = pipeline.run_complete_pipeline(
                crop_size=args.crop_size,
                gap=args.gap,
                rates=tuple(args.rates),
                force_slice=args.force_slice,
                epochs=args.epochs,
                imgsz=args.imgsz,
                batch=args.batch,
                device=args.device,
                resume=args.resume
            )
        
        if not success:
            sys.exit(1)
            
    except Exception as e:
        LOGGER.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
