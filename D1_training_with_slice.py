#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
D1 æ•°æ®åˆ‡ç‰‡ + è®­ç»ƒé›†æˆè„šæœ¬ (å•å°ºåº¦ç‰ˆæœ¬)
å°†æ•°æ®åˆ‡ç‰‡å’Œæ¨¡å‹è®­ç»ƒé›†æˆåˆ°ä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµä¸­
"""

import argparse
import sys
import time
from pathlib import Path
from typing import Tuple, Optional

from ultralytics import YOLO
from ultralytics.data.split_yolo import split_trainval
from ultralytics.utils import LOGGER


class BalloonTrainingPipeline:
    """Balloon æ•°æ®åˆ‡ç‰‡å’Œè®­ç»ƒæµæ°´çº¿"""
    
    def __init__(
        self,
        data_root: str,
        slice_dir: str,
        model_name: str = "yolo11n.pt",
        project_name: str = "D1_yolo11n_slice"
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒæµæ°´çº¿
        
        Args:
            data_root (str): åŸå§‹ Balloon æ•°æ®æ ¹ç›®å½•
            slice_dir (str): åˆ‡ç‰‡åæ•°æ®ä¿å­˜ç›®å½•
            model_name (str): æ¨¡å‹åç§°æˆ–è·¯å¾„
            project_name (str): è®­ç»ƒé¡¹ç›®åç§°
        """
        self.data_root = Path(data_root)
        self.slice_dir = Path(slice_dir)
        self.model_name = model_name
        self.project_name = project_name
        
        # éªŒè¯è·¯å¾„
        if not self.data_root.exists():
            raise ValueError(f"æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {self.data_root}")
    
    def check_data_structure(self) -> bool:
        """æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„"""
        LOGGER.info("ğŸ” æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„...")
        
        required_dirs = [
            "images/train",
            "images/val",
            "labels/train", 
            "labels/val"
        ]
        
        missing_dirs = []
        for dir_path in required_dirs:
            full_path = self.data_root / dir_path
            if not full_path.exists():
                missing_dirs.append(str(full_path))
        
        if missing_dirs:
            LOGGER.error("âŒ æ•°æ®ç›®å½•ç»“æ„ä¸å®Œæ•´")
            for missing_dir in missing_dirs:
                LOGGER.error(f"  ç¼ºå°‘: {missing_dir}")
            return False
        
        LOGGER.info("âœ… æ•°æ®ç›®å½•ç»“æ„æ£€æŸ¥é€šè¿‡")
        return True
    
    def get_dataset_stats(self, data_path: Path) -> dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for split in ["train", "val"]:
            img_dir = data_path / "images" / split
            lbl_dir = data_path / "labels" / split
            
            img_count = len(list(img_dir.glob("*.*"))) if img_dir.exists() else 0
            lbl_count = len(list(lbl_dir.glob("*.txt"))) if lbl_dir.exists() else 0
            
            stats[split] = {"images": img_count, "labels": lbl_count}
        
        return stats
    
    def slice_data(
        self,
        crop_size: int = 640,
        gap: int = 100,
        rates: Tuple[float, ...] = (1.0,),
        force_slice: bool = False
    ) -> bool:
        """
        æ‰§è¡Œæ•°æ®åˆ‡ç‰‡
        
        Args:
            crop_size (int): åˆ‡ç‰‡çª—å£å¤§å°
            gap (int): çª—å£é‡å å¤§å°
            rates (Tuple[float, ...]): å¤šå°ºåº¦ç¼©æ”¾æ¯”ä¾‹
            force_slice (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ‡ç‰‡
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        LOGGER.info("ğŸ“¸ å¼€å§‹æ•°æ®åˆ‡ç‰‡...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ‡ç‰‡
        if self.slice_dir.exists() and not force_slice:
            slice_stats = self.get_dataset_stats(self.slice_dir)
            if slice_stats["train"]["images"] > 0:
                LOGGER.info(f"âœ… å‘ç°å·²åˆ‡ç‰‡çš„æ•°æ®: {self.slice_dir}")
                LOGGER.info(f"   è®­ç»ƒé›†: {slice_stats['train']['images']} å›¾åƒ")
                LOGGER.info(f"   éªŒè¯é›†: {slice_stats['val']['images']} å›¾åƒ")
                LOGGER.info("   ä½¿ç”¨ --force-slice å¼ºåˆ¶é‡æ–°åˆ‡ç‰‡")
                return True
        
        # è·å–åŸå§‹æ•°æ®ç»Ÿè®¡
        orig_stats = self.get_dataset_stats(self.data_root)
        LOGGER.info(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
        for split, stats in orig_stats.items():
            LOGGER.info(f"  {split}: {stats['images']} å›¾åƒ, {stats['labels']} æ ‡ç­¾")
        
        LOGGER.info(f"åˆ‡ç‰‡å‚æ•°:")
        LOGGER.info(f"  çª—å£å¤§å°: {crop_size}x{crop_size}")
        LOGGER.info(f"  é‡å å¤§å°: {gap}")
        LOGGER.info(f"  ç¼©æ”¾æ¯”ä¾‹: {rates}")
        
        try:
            start_time = time.time()
            
            # æ‰§è¡Œåˆ‡ç‰‡
            split_trainval(
                data_root=str(self.data_root),
                save_dir=str(self.slice_dir),
                crop_size=crop_size,
                gap=gap,
                rates=rates
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # è·å–åˆ‡ç‰‡åç»Ÿè®¡
            slice_stats = self.get_dataset_stats(self.slice_dir)
            LOGGER.info(f"âœ… æ•°æ®åˆ‡ç‰‡å®Œæˆ (è€—æ—¶: {duration:.1f}s)")
            LOGGER.info(f"åˆ‡ç‰‡åæ•°æ®ç»Ÿè®¡:")
            for split, stats in slice_stats.items():
                LOGGER.info(f"  {split}: {stats['images']} å›¾åƒ, {stats['labels']} æ ‡ç­¾")
            
            return True
            
        except Exception as e:
            LOGGER.error(f"âŒ æ•°æ®åˆ‡ç‰‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def train_model(
        self,
        epochs: int = 5,
        imgsz: int = 640,
        batch: int = 16,
        device: int = 0,
        patience: int = 30,
        resume: bool = False
    ) -> Optional[str]:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            epochs (int): è®­ç»ƒè½®æ•°
            imgsz (int): è¾“å…¥å›¾åƒå°ºå¯¸
            batch (int): æ‰¹æ¬¡å¤§å°
            device (int): GPU è®¾å¤‡ç¼–å·
            patience (int): æ—©åœè€å¿ƒå€¼
            resume (bool): æ˜¯å¦æ¢å¤è®­ç»ƒ
        
        Returns:
            str: æœ€ä½³æ¨¡å‹è·¯å¾„
        """
        LOGGER.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        
        # åˆ›å»ºä¸´æ—¶æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨å®é™…çš„åˆ‡ç‰‡ç›®å½•
        import yaml
        import tempfile
        
        dataset_config = {
            'path': str(self.slice_dir.absolute()),
            'train': 'images/train',
            'val': 'images/val',
            'nc': 3,
            'names': {0: 'hole', 1: 'cave', 2: 'unknow'}
        }
        
        # ä¿å­˜ä¸´æ—¶yamlæ–‡ä»¶
        temp_yaml = Path(tempfile.gettempdir()) / f"{self.project_name}_data.yaml"
        with open(temp_yaml, 'w') as f:
            yaml.dump(dataset_config, f)
        
        dataset_yaml = str(temp_yaml)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = YOLO(self.model_name)
        LOGGER.info(f"æ¨¡å‹: {self.model_name}")
        LOGGER.info(f"æ•°æ®: {dataset_yaml} -> {self.slice_dir}")
        LOGGER.info(f"è®­ç»ƒå‚æ•°: epochs={epochs}, imgsz={imgsz}, batch={batch}, patience={patience}")
        
        try:
            # å¼€å§‹è®­ç»ƒ
            results = model.train(
                data=str(dataset_yaml),

                # === åŸºç¡€è®­ç»ƒå‚æ•° ===
                epochs=epochs,            # è®­ç»ƒè½®æ•°
                imgsz=imgsz,              # å›¾åƒå°ºå¯¸
                batch=batch,              # æ‰¹å¤§å°
                device=device,            # GPUè®¾å¤‡
                
                # === é¡¹ç›®ç®¡ç† ===
                project="runs/detect",
                name=f"{self.project_name}",
                resume=resume,
                exist_ok=True,            # å…è®¸è¦†ç›–ç°æœ‰å®éªŒ

                # === æ—©åœå’Œä¿å­˜ ===
                patience=patience,        # æ—©åœè€å¿ƒå€¼
                save_period=20,           # æ¯20è½®ä¿å­˜ä¸€æ¬¡

                # === è®­ç»ƒä¼˜åŒ– ===
                amp=True,                 # å¯ç”¨AMP
                cache=False,              # ä¸ç¼“å­˜å›¾åƒ
                rect=False,               # ä¸ä½¿ç”¨çŸ©å½¢è®­ç»ƒ
                cos_lr=True,              # ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
                lr0=0.005,                 # åˆå§‹å­¦ä¹ ç‡
                lrf=0.02,                 # æœ€ç»ˆå­¦ä¹ ç‡å› å­

                # === å…¶ä»–è®¾ç½® ===
                workers=4,                # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
                verbose=True,             # è¯¦ç»†è¾“å‡º
                seed=42,                  # éšæœºç§å­ï¼Œä¿è¯å¯é‡ç°æ€§
                deterministic=True,       # ç¡®å®šæ€§è®­ç»ƒ
                single_cls=False,         # å•ç±»åˆ«è®­ç»ƒ
                plots=True,               # ç”Ÿæˆè®­ç»ƒå›¾è¡¨

                # === æ•°æ®å¢å¼ºè®¾ç½® ===
                degrees=15.0,             # æ—‹è½¬è§’åº¦
                flipud=0.0,               # ç«–ç›´ç¿»è½¬
                fliplr=0.5,               # æ°´å¹³ç¿»è½¬
                mosaic=1.0,               # Mosaicå¢å¼º
                close_mosaic=10,          # å…³é—­Mosaicçš„epoch
                mixup=0.0,                # æ··åˆå¢å¼º
                erasing=0.1,              # éšæœºæ“¦é™¤
                translate=0.1,            # å¹³ç§»å¢å¼º
                scale=0.5,                # ç¼©æ”¾å¢å¼º
                
                # === æŸå¤±å‡½æ•°æƒé‡ ===
                box=7.5,                  # è¾¹ç•Œæ¡†æŸå¤±æƒé‡
                cls=0.5,                  # åˆ†ç±»æŸå¤±æƒé‡

                # === éªŒè¯è®¾ç½® ===
                val=True,                 # è®­ç»ƒæ—¶è¿›è¡ŒéªŒè¯
                split='val',              # éªŒè¯é›†åˆ†å‰²
                save_json=False,          # ä¸ä¿å­˜JSONç»“æœ
                save_hybrid=False,        # ä¸ä¿å­˜hybridæ ‡ç­¾
            )
            
            LOGGER.info("âœ… è®­ç»ƒå®Œæˆï¼")
            
            # ä»è®­ç»ƒç»“æœä¸­è·å–å®é™…ä¿å­˜ç›®å½•
            save_dir = Path(model.trainer.save_dir)
            LOGGER.info(f"è®­ç»ƒç»“æœä¿å­˜åœ¨: {save_dir}")
            
            # è¿”å›æœ€ä½³æ¨¡å‹è·¯å¾„
            best_model = save_dir / "weights" / "best.pt"
            last_model = save_dir / "weights" / "last.pt"
            
            if best_model.exists():
                LOGGER.info(f"âœ… æœ€ä½³æ¨¡å‹: {best_model}")
                return str(best_model)
            elif last_model.exists():
                LOGGER.info(f"âš ï¸ æœªæ‰¾åˆ°best.ptï¼Œè¿”å›last.pt: {last_model}")
                return str(last_model)
            else:
                LOGGER.warning(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨äº: {save_dir / 'weights'}")
                # å°è¯•åœ¨runs/detectä¸‹æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç›®å½•
                runs_dir = Path("runs/detect")
                if runs_dir.exists():
                    subdirs = [d for d in runs_dir.iterdir() if d.is_dir()]
                    if subdirs:
                        latest_dir = max(subdirs, key=lambda x: x.stat().st_mtime)
                        LOGGER.info(f"å°è¯•ä»æœ€æ–°è®­ç»ƒç›®å½•è·å–: {latest_dir}")
                        best_model = latest_dir / "weights" / "best.pt"
                        last_model = latest_dir / "weights" / "last.pt"
                        if best_model.exists():
                            return str(best_model)
                        elif last_model.exists():
                            return str(last_model)
            
            return None
            
        except Exception as e:
            LOGGER.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_complete_pipeline(
        self,
        # åˆ‡ç‰‡å‚æ•°
        crop_size: int = 640,
        gap: int = 100,
        rates: Tuple[float, ...] = (1.0,),
        force_slice: bool = False,
        # è®­ç»ƒå‚æ•°
        epochs: int = 100,
        imgsz: int = 640,
        batch: int = 16,
        device: int = 0,
        patience: int = 30,
        resume: bool = False
    ) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        LOGGER.info("ğŸ¯ å¼€å§‹ Balloon å®Œæ•´è®­ç»ƒæµæ°´çº¿")
        LOGGER.info(f"é¡¹ç›®åç§°: {self.project_name}")
        LOGGER.info(f"åŸå§‹æ•°æ®: {self.data_root}")
        LOGGER.info(f"åˆ‡ç‰‡æ•°æ®: {self.slice_dir}")
        
        # 1. æ£€æŸ¥æ•°æ®ç»“æ„
        if not self.check_data_structure():
            return False
        
        # 2. æ•°æ®åˆ‡ç‰‡
        if not self.slice_data(crop_size, gap, rates, force_slice):
            return False
        
        # 3. æ¨¡å‹è®­ç»ƒ
        best_model = self.train_model(epochs, imgsz, batch, device, patience, resume)
        if best_model is None:
            return False
        
        LOGGER.info("ğŸ‰ å®Œæ•´è®­ç»ƒæµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
        LOGGER.info(f"æœ€ä½³æ¨¡å‹: {best_model}")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Balloon æ•°æ®åˆ‡ç‰‡å’Œè®­ç»ƒé›†æˆè„šæœ¬ (å•å°ºåº¦ç‰ˆæœ¬)")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data-root", type=str, 
                       default="/public/home/baichen/download/dcu_yolo/ultralytics/data/D1_type3/yolo_format",
                       help="åŸå§‹ Balloon æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--slice-dir", type=str, 
                       default="/public/home/baichen/download/dcu_yolo/ultralytics/data/D1_type3/yolo_format_slice",
                       help="åˆ‡ç‰‡åæ•°æ®ä¿å­˜ç›®å½•")
    parser.add_argument("--project-name", type=str, default="D1_yolo11l_slice", help="è®­ç»ƒé¡¹ç›®åç§°")
    
    # åˆ‡ç‰‡å‚æ•°
    parser.add_argument("--crop-size", type=int, default=640, help="åˆ‡ç‰‡çª—å£å¤§å°")
    parser.add_argument("--gap", type=int, default=100, help="çª—å£é‡å å¤§å°")
    parser.add_argument("--rates", nargs="+", type=float, default=[1.0], help="å¤šå°ºåº¦ç¼©æ”¾æ¯”ä¾‹")
    parser.add_argument("--force-slice", action="store_true", help="å¼ºåˆ¶é‡æ–°åˆ‡ç‰‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--model", type=str, default="yolo11l.pt", help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=200, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--imgsz", type=int, default=640, help="è¾“å…¥å›¾åƒå°ºå¯¸")
    parser.add_argument("--batch", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--device", type=int, default=0, help="GPU è®¾å¤‡ç¼–å·")
    parser.add_argument("--patience", type=int, default=30, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--resume", action="store_true", help="æ¢å¤è®­ç»ƒ")
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument("--slice-only", action="store_true", help="ä»…æ‰§è¡Œæ•°æ®åˆ‡ç‰‡")
    parser.add_argument("--train-only", action="store_true", help="ä»…æ‰§è¡Œæ¨¡å‹è®­ç»ƒ")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºè®­ç»ƒæµæ°´çº¿
        pipeline = BalloonTrainingPipeline(
            data_root=args.data_root,
            slice_dir=args.slice_dir,
            model_name=args.model,
            project_name=args.project_name
        )
        
        if args.slice_only:
            # ä»…æ‰§è¡Œåˆ‡ç‰‡
            success = pipeline.slice_data(
                crop_size=args.crop_size,
                gap=args.gap,
                rates=tuple(args.rates),
                force_slice=args.force_slice
            )
        elif args.train_only:
            # ä»…æ‰§è¡Œè®­ç»ƒ
            best_model = pipeline.train_model(
                epochs=args.epochs,
                imgsz=args.imgsz,
                batch=args.batch,
                device=args.device,
                patience=args.patience,
                resume=args.resume
            )
            success = best_model is not None
        else:
            # æ‰§è¡Œå®Œæ•´æµæ°´çº¿
            success = pipeline.run_complete_pipeline(
                crop_size=args.crop_size,
                gap=args.gap,
                rates=tuple(args.rates),
                force_slice=args.force_slice,
                epochs=args.epochs,
                imgsz=args.imgsz,
                batch=args.batch,
                device=args.device,
                patience=args.patience,
                resume=args.resume
            )
        
        if not success:
            sys.exit(1)
            
    except Exception as e:
        LOGGER.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

