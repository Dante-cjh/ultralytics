#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸¤é˜¶æ®µçº§è”æ£€æµ‹ - æ‰¹é‡æ¨ç†å’Œè¯„ä¼°è„šæœ¬

åŠŸèƒ½ï¼š
1. åœ¨æ•´ä¸ªéªŒè¯é›†ä¸Šè¿›è¡Œä¸¤é˜¶æ®µæ¨ç†
2. è®¡ç®—æ•°é‡å‡†ç¡®ç‡æŒ‡æ ‡ (1 - |predict-true|/true)
3. ä¸å•é˜¶æ®µYOLOå¯¹æ¯”
4. ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
"""

import os
import sys
import cv2
import yaml
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import argparse
from tqdm import tqdm
from ultralytics import YOLO
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from collections import defaultdict

# å¯¼å…¥åˆ†ç±»å™¨æ¨¡å‹
sys.path.insert(0, str(Path(__file__).parent))
from balloon_cascaded_detection import MobileNetClassifier, SimpleMLP, parse_yolo_label, cross_class_nms


class CascadedEvaluator:
    """ä¸¤é˜¶æ®µçº§è”æ£€æµ‹è¯„ä¼°å™¨"""
    
    def __init__(self, yolo_model_path: str, classifier_path: str,
                 classifier_type: str = "mobilenet", input_size: int = 112,
                 num_classes: int = 2, device: str = "cuda:0",
                 cross_class_nms: bool = True, nms_iou: float = 0.3):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            classifier_path: åˆ†ç±»å™¨æƒé‡è·¯å¾„
            classifier_type: åˆ†ç±»å™¨ç±»å‹
            input_size: åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸
            num_classes: ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
            device: è®¾å¤‡
            cross_class_nms: æ˜¯å¦ä½¿ç”¨è·¨ç±»åˆ«NMS
            nms_iou: è·¨ç±»åˆ«NMSçš„IOUé˜ˆå€¼
        """
        # åŠ è½½YOLOæ¨¡å‹
        self.yolo_model = YOLO(yolo_model_path)
        self.yolo_model.to(device)
        
        # åŠ è½½åˆ†ç±»å™¨
        if classifier_type == "mlp":
            self.classifier = SimpleMLP(input_size, num_classes)
        else:
            self.classifier = MobileNetClassifier(num_classes)
        
        self.classifier.load_state_dict(torch.load(classifier_path, map_location=device))
        self.classifier.to(device)
        self.classifier.eval()
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((input_size, input_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        self.device = device
        self.use_cross_class_nms = cross_class_nms
        self.nms_iou = nms_iou
        
        print(f"âœ… åŠ è½½ä¸¤é˜¶æ®µæ£€æµ‹å™¨")
        print(f"   YOLO: {yolo_model_path}")
        print(f"   åˆ†ç±»å™¨: {classifier_path}")
        print(f"   è·¨ç±»åˆ«NMS: {'å¯ç”¨' if cross_class_nms else 'ç¦ç”¨'} (IOU={nms_iou})")
    
    def detect_single_stage(self, image_path: str, conf: float = 0.25, 
                           imgsz: int = 1280) -> List[Dict]:
        """
        å•é˜¶æ®µYOLOæ£€æµ‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        
        Returns:
            [{'box': [x1,y1,x2,y2], 'cls': int, 'conf': float}, ...]
        """
        results = self.yolo_model.predict(
            source=image_path,
            imgsz=imgsz,
            conf=conf,
            iou=0.45,
            device=self.device,
            verbose=False,
            save=False,
        )
        
        result = results[0]
        detections = []
        
        if len(result.boxes) > 0:
            xyxy = result.boxes.xyxy.cpu().numpy()
            cls = result.boxes.cls.cpu().numpy()
            conf_scores = result.boxes.conf.cpu().numpy()
            
            for i in range(len(xyxy)):
                detections.append({
                    'box': xyxy[i].tolist(),
                    'cls': int(cls[i]),
                    'conf': float(conf_scores[i])
                })
        
        return detections
    
    def detect_two_stage(self, image_path: str, stage1_conf: float = 0.05,
                        stage2_threshold: float = 0.5, imgsz: int = 1280) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µçº§è”æ£€æµ‹
        
        Returns:
            [{'box': [x1,y1,x2,y2], 'cls': int, 'conf': float, 'stage1_cls': int, 'stage1_conf': float}, ...]
        """
        # è¯»å–å›¾åƒ
        img = cv2.imread(image_path)
        img_h, img_w = img.shape[:2]
        
        # ç¬¬ä¸€é˜¶æ®µï¼šYOLOç”Ÿæˆå€™é€‰æ¡†
        results = self.yolo_model.predict(
            source=image_path,
            imgsz=imgsz,
            conf=stage1_conf,
            iou=0.45,
            device=self.device,
            verbose=False,
            save=False,
        )
        
        result = results[0]
        detections = []
        
        if len(result.boxes) == 0:
            return detections
        
        xyxy = result.boxes.xyxy.cpu().numpy()
        cls = result.boxes.cls.cpu().numpy()
        conf = result.boxes.conf.cpu().numpy()
        
        # ç¬¬äºŒé˜¶æ®µï¼šåˆ†ç±»å™¨é‡åˆ†ç±»
        for i in range(len(xyxy)):
            x1, y1, x2, y2 = xyxy[i]
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            
            # è¾¹ç•Œæ£€æŸ¥
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(img_w, x2)
            y2 = min(img_h, y2)
            
            if x2 <= x1 or y2 <= y1:
                continue
            
            # è£å‰ªå€™é€‰åŒºåŸŸ
            crop = img[y1:y2, x1:x2]
            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
            
            # åˆ†ç±»å™¨æ¨ç†
            crop_tensor = self.transform(crop_pil).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                output = self.classifier(crop_tensor)
                probs = F.softmax(output, dim=1)
                stage2_conf, stage2_cls = probs.max(1)
                stage2_conf = stage2_conf.item()
                stage2_cls = stage2_cls.item()
            
            # è¿‡æ»¤ï¼šå¦‚æœåˆ†ç±»å™¨åˆ¤æ–­ä¸ºèƒŒæ™¯ï¼ˆcls=0ï¼‰æˆ–ç½®ä¿¡åº¦ä½ï¼Œåˆ™ä¸¢å¼ƒ
            if stage2_cls == 0 or stage2_conf < stage2_threshold:
                continue
            
            # è½¬æ¢ç±»åˆ«ï¼ˆåˆ†ç±»å™¨çš„ç±»åˆ«ä»1å¼€å§‹ï¼Œéœ€è¦å‡1ï¼‰
            final_cls = stage2_cls - 1
            
            detections.append({
                'box': [float(x1), float(y1), float(x2), float(y2)],
                'cls': final_cls,
                'conf': stage2_conf,
                'stage1_cls': int(cls[i]),
                'stage1_conf': float(conf[i])
            })
        
        # è·¨ç±»åˆ«NMSï¼ˆå¯é€‰ï¼‰
        if self.use_cross_class_nms and len(detections) > 0:
            detections = cross_class_nms(detections, self.nms_iou)
        
        return detections
    
    def count_objects(self, detections: List[Dict]) -> Dict[int, int]:
        """ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡"""
        counts = defaultdict(int)
        for det in detections:
            counts[det['cls']] += 1
        return dict(counts)
    
    def count_gt_objects(self, label_path: str, img_w: int, img_h: int) -> Dict[int, int]:
        """ç»Ÿè®¡GTä¸­æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡"""
        gt_boxes = parse_yolo_label(label_path, img_w, img_h)
        counts = defaultdict(int)
        for box in gt_boxes:
            counts[int(box[0])] += 1
        return dict(counts)
    
    def calculate_count_accuracy(self, pred_counts: Dict[int, int], 
                                 gt_counts: Dict[int, int]) -> float:
        """
        è®¡ç®—æ•°é‡å‡†ç¡®ç‡: 1 - |predict-true|/true
        
        Args:
            pred_counts: é¢„æµ‹çš„æ¯ç±»æ•°é‡ {cls: count}
            gt_counts: GTçš„æ¯ç±»æ•°é‡ {cls: count}
        
        Returns:
            æ•°é‡å‡†ç¡®ç‡ (0-1)
        """
        # è·å–æ‰€æœ‰ç±»åˆ«
        all_classes = set(list(pred_counts.keys()) + list(gt_counts.keys()))
        
        total_error = 0
        total_gt = 0
        
        for cls in all_classes:
            pred = pred_counts.get(cls, 0)
            gt = gt_counts.get(cls, 0)
            
            if gt > 0:
                error = abs(pred - gt) / gt
                total_error += error * gt
                total_gt += gt
        
        if total_gt == 0:
            return 1.0
        
        accuracy = 1.0 - (total_error / total_gt)
        return max(0.0, accuracy)  # ç¡®ä¿ä¸å°äº0
    
    def evaluate_dataset(self, data_yaml: str, split: str = "val",
                        stage1_conf: float = 0.05, stage2_threshold: float = 0.5,
                        yolo_conf: float = 0.25, imgsz: int = 1280,
                        save_dir: str = "runs/cascaded_eval"):
        """
        åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°
        
        Args:
            data_yaml: æ•°æ®é›†YAMLé…ç½®
            split: 'train' æˆ– 'val'
            stage1_conf: ç¬¬ä¸€é˜¶æ®µç½®ä¿¡åº¦
            stage2_threshold: ç¬¬äºŒé˜¶æ®µé˜ˆå€¼
            yolo_conf: å•é˜¶æ®µYOLOç½®ä¿¡åº¦ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            imgsz: æ¨ç†å°ºå¯¸
            save_dir: ä¿å­˜ç›®å½•
        """
        # è¯»å–æ•°æ®é›†é…ç½®
        with open(data_yaml, 'r') as f:
            data_config = yaml.safe_load(f)
        
        dataset_path = Path(data_config['path'])
        # æ”¯æŒä¸¤ç§ç›®å½•ç»“æ„ï¼š
        # 1. path/train/images å’Œ path/train/labels
        # 2. path/images/train å’Œ path/labels/train (balloonæ ¼å¼)
        if (dataset_path / 'images' / split).exists():
            # Balloonæ ¼å¼
            image_dir = dataset_path / 'images' / split
            label_dir = dataset_path / 'labels' / split
        else:
            # æ ‡å‡†æ ¼å¼
            image_dir = dataset_path / data_config[split] / 'images'
            label_dir = dataset_path / data_config[split] / 'labels'
        
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆç±»ä¼¼runs/inference/<model_name>_valçš„ç»“æ„ï¼‰
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºimageså’Œlabelsç›®å½•
        images_dir = save_path / 'images'
        images_dir.mkdir(exist_ok=True)
        labels_dir_single = save_path / 'labels_single_stage'
        labels_dir_two = save_path / 'labels_two_stage'
        labels_dir_single.mkdir(exist_ok=True)
        labels_dir_two.mkdir(exist_ok=True)
        
        # é¢å¤–åˆ›å»ºå¯è§†åŒ–å¯¹æ¯”ç›®å½•
        vis_comp_dir = save_path / 'visualizations_comparison'
        vis_comp_dir.mkdir(exist_ok=True)
        
        # ç»“æœå­˜å‚¨
        results = {
            'single_stage': [],
            'two_stage': [],
            'comparison': []
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'single_stage': {
                'count_accuracies': [],
                'total_detections': 0,
                'total_gt': 0
            },
            'two_stage': {
                'count_accuracies': [],
                'total_detections': 0,
                'total_gt': 0
            }
        }
        
        # å¤„ç†æ¯å¼ å›¾åƒ
        image_files = sorted(image_dir.glob("*.jpg")) + sorted(image_dir.glob("*.png"))
        
        print(f"\nğŸ” è¯„ä¼° {split} é›† ({len(image_files)} å¼ å›¾åƒ)...")
        print(f"   å•é˜¶æ®µYOLOç½®ä¿¡åº¦: {yolo_conf}")
        print(f"   ä¸¤é˜¶æ®µé…ç½®: stage1_conf={stage1_conf}, stage2_threshold={stage2_threshold}")
        
        for img_path in tqdm(image_files, desc="è¯„ä¼°ä¸­"):
            # è¯»å–å›¾åƒ
            img = cv2.imread(str(img_path))
            if img is None:
                continue
            
            img_h, img_w = img.shape[:2]
            
            # è¯»å–GT
            label_path = label_dir / (img_path.stem + '.txt')
            gt_counts = self.count_gt_objects(str(label_path), img_w, img_h)
            total_gt = sum(gt_counts.values())
            
            # å•é˜¶æ®µæ£€æµ‹
            single_dets = self.detect_single_stage(str(img_path), yolo_conf, imgsz)
            single_counts = self.count_objects(single_dets)
            single_acc = self.calculate_count_accuracy(single_counts, gt_counts)
            
            # ä¸¤é˜¶æ®µæ£€æµ‹
            two_dets = self.detect_two_stage(str(img_path), stage1_conf, stage2_threshold, imgsz)
            two_counts = self.count_objects(two_dets)
            two_acc = self.calculate_count_accuracy(two_counts, gt_counts)
            
            # è®°å½•ç»“æœ
            results['single_stage'].append({
                'image': img_path.name,
                'gt_counts': gt_counts,
                'pred_counts': single_counts,
                'count_accuracy': single_acc,
                'num_detections': len(single_dets)
            })
            
            results['two_stage'].append({
                'image': img_path.name,
                'gt_counts': gt_counts,
                'pred_counts': two_counts,
                'count_accuracy': two_acc,
                'num_detections': len(two_dets)
            })
            
            results['comparison'].append({
                'image': img_path.name,
                'gt_total': total_gt,
                'single_stage': {
                    'count': sum(single_counts.values()),
                    'accuracy': single_acc
                },
                'two_stage': {
                    'count': sum(two_counts.values()),
                    'accuracy': two_acc
                },
                'improvement': two_acc - single_acc
            })
            
            # æ›´æ–°ç»Ÿè®¡
            stats['single_stage']['count_accuracies'].append(single_acc)
            stats['single_stage']['total_detections'] += len(single_dets)
            stats['single_stage']['total_gt'] += total_gt
            
            stats['two_stage']['count_accuracies'].append(two_acc)
            stats['two_stage']['total_detections'] += len(two_dets)
            stats['two_stage']['total_gt'] += total_gt
            
            # ä¿å­˜æ¨ç†å›¾åƒã€å¯è§†åŒ–å’Œæ ‡ç­¾
            self._save_visualizations_and_labels(
                img, img_path.stem, single_dets, two_dets,
                images_dir, vis_comp_dir, labels_dir_single, labels_dir_two
            )
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        single_avg_acc = np.mean(stats['single_stage']['count_accuracies']) * 100
        two_avg_acc = np.mean(stats['two_stage']['count_accuracies']) * 100
        improvement = two_avg_acc - single_avg_acc
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(save_path / 'detailed_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(results, stats, single_avg_acc, two_avg_acc)
        
        with open(save_path / 'evaluation_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        print(f"\nâœ… è¯„ä¼°å®Œæˆ! ç»“æœä¿å­˜è‡³: {save_path}")
    
    def _save_visualizations_and_labels(self, img: np.ndarray, img_name: str,
                                        single_dets: List[Dict], two_dets: List[Dict],
                                        images_dir: Path, vis_comp_dir: Path,
                                        labels_dir_single: Path, labels_dir_two: Path):
        """
        ä¿å­˜æ¨ç†å›¾åƒã€å¯è§†åŒ–å¯¹æ¯”å’ŒYOLOæ ¼å¼æ ‡ç­¾
        
        Args:
            img: åŸå§‹å›¾åƒ
            img_name: å›¾åƒåç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
            single_dets: å•é˜¶æ®µæ£€æµ‹ç»“æœ
            two_dets: ä¸¤é˜¶æ®µæ£€æµ‹ç»“æœ
            images_dir: æ¨ç†å›¾åƒä¿å­˜ç›®å½•
            vis_comp_dir: å¯è§†åŒ–å¯¹æ¯”ä¿å­˜ç›®å½•
            labels_dir_single: å•é˜¶æ®µæ ‡ç­¾ä¿å­˜ç›®å½•
            labels_dir_two: ä¸¤é˜¶æ®µæ ‡ç­¾ä¿å­˜ç›®å½•
        """
        img_h, img_w = img.shape[:2]
        
        # ç»˜åˆ¶ä¸¤é˜¶æ®µæ¨ç†å›¾åƒï¼ˆä¸»è¦ç»“æœï¼Œä¿å­˜åˆ°imagesç›®å½•ï¼‰
        img_two_stage = img.copy()
        for det in two_dets:
            x1, y1, x2, y2 = [int(v) for v in det['box']]
            cv2.rectangle(img_two_stage, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label = f"cls{det['cls']} {det['conf']:.2f}"
            cv2.putText(img_two_stage, label, (x1, y1-5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ä¿å­˜ä¸¤é˜¶æ®µæ¨ç†å›¾åƒåˆ°imagesç›®å½•ï¼ˆä¸»è¦ç»“æœï¼‰
        cv2.imwrite(str(images_dir / f"{img_name}.jpg"), img_two_stage)
        
        # ç»˜åˆ¶å¯¹æ¯”å¯è§†åŒ–ï¼ˆå·¦ï¼šå•é˜¶æ®µï¼Œå³ï¼šä¸¤é˜¶æ®µï¼‰
        img_single = img.copy()
        img_two = img.copy()
        
        # ç»˜åˆ¶å•é˜¶æ®µæ£€æµ‹ç»“æœ
        for det in single_dets:
            x1, y1, x2, y2 = [int(v) for v in det['box']]
            cv2.rectangle(img_single, (x1, y1), (x2, y2), (0, 0, 255), 2)  # çº¢è‰²
            label = f"cls{det['cls']} {det['conf']:.2f}"
            cv2.putText(img_single, label, (x1, y1-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ç»˜åˆ¶ä¸¤é˜¶æ®µæ£€æµ‹ç»“æœï¼ˆç»¿è‰²ï¼‰
        for det in two_dets:
            x1, y1, x2, y2 = [int(v) for v in det['box']]
            cv2.rectangle(img_two, (x1, y1), (x2, y2), (0, 255, 0), 2)  # ç»¿è‰²
            label = f"cls{det['cls']} {det['conf']:.2f}"
            cv2.putText(img_two, label, (x1, y1-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # æ‹¼æ¥ä¸¤å¼ å›¾åƒï¼ˆå¯¹æ¯”å¯è§†åŒ–ï¼‰
        gap = np.ones((img_h, 20, 3), dtype=np.uint8) * 255
        vis_img = np.hstack([img_single, gap, img_two])
        
        # æ·»åŠ æ ‡é¢˜
        title_height = 50
        title_bar = np.ones((title_height, vis_img.shape[1], 3), dtype=np.uint8) * 255
        cv2.putText(title_bar, f"Single-Stage ({len(single_dets)} dets)", 
                   (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)  # çº¢è‰²
        cv2.putText(title_bar, f"Two-Stage ({len(two_dets)} dets)", 
                   (img_w + 30, 35), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)  # ç»¿è‰²
        
        vis_img = np.vstack([title_bar, vis_img])
        
        # ä¿å­˜å¯¹æ¯”å¯è§†åŒ–å›¾åƒåˆ°visualizations_comparisonç›®å½•
        cv2.imwrite(str(vis_comp_dir / f"{img_name}_comparison.jpg"), vis_img)
        
        # ä¿å­˜YOLOæ ¼å¼æ ‡ç­¾
        # å•é˜¶æ®µ
        with open(labels_dir_single / f"{img_name}.txt", 'w') as f:
            for det in single_dets:
                x1, y1, x2, y2 = det['box']
                cls = det['cls']
                # è½¬æ¢ä¸ºYOLOæ ¼å¼ (class x_center y_center width height)
                x_center = (x1 + x2) / 2 / img_w
                y_center = (y1 + y2) / 2 / img_h
                width = (x2 - x1) / img_w
                height = (y2 - y1) / img_h
                f.write(f"{cls} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
        
        # ä¸¤é˜¶æ®µ
        with open(labels_dir_two / f"{img_name}.txt", 'w') as f:
            for det in two_dets:
                x1, y1, x2, y2 = det['box']
                cls = det['cls']
                # è½¬æ¢ä¸ºYOLOæ ¼å¼
                x_center = (x1 + x2) / 2 / img_w
                y_center = (y1 + y2) / 2 / img_h
                width = (x2 - x1) / img_w
                height = (y2 - y1) / img_h
                f.write(f"{cls} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
    
    def generate_report(self, results: Dict, stats: Dict, 
                       single_avg_acc: float, two_avg_acc: float) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        improvement = two_avg_acc - single_avg_acc
        
        report = f"""
{'='*60}
ä¸¤é˜¶æ®µçº§è”æ£€æµ‹ - è¯„ä¼°æŠ¥å‘Š
{'='*60}

ã€æ€»ä½“æŒ‡æ ‡ã€‘

å•é˜¶æ®µYOLO:
  - å¹³å‡æ•°é‡å‡†ç¡®ç‡: {single_avg_acc:.2f}%
  - æ€»æ£€æµ‹æ•°: {stats['single_stage']['total_detections']}
  - æ€»GTæ•°: {stats['single_stage']['total_gt']}

ä¸¤é˜¶æ®µçº§è”:
  - å¹³å‡æ•°é‡å‡†ç¡®ç‡: {two_avg_acc:.2f}%
  - æ€»æ£€æµ‹æ•°: {stats['two_stage']['total_detections']}
  - æ€»GTæ•°: {stats['two_stage']['total_gt']}

æ€§èƒ½æå‡:
  - å‡†ç¡®ç‡æå‡: {improvement:+.2f}%
  - {'âœ… ä¸¤é˜¶æ®µæ›´ä¼˜' if improvement > 0 else 'âŒ å•é˜¶æ®µæ›´ä¼˜'}

{'='*60}

ã€è¯¦ç»†åˆ†æã€‘

"""
        
        # æ‰¾å‡ºæå‡æœ€å¤§çš„å›¾åƒ
        improvements = [(r['image'], r['improvement']) 
                       for r in results['comparison']]
        improvements.sort(key=lambda x: x[1], reverse=True)
        
        report += "æå‡æœ€å¤§çš„å‰5å¼ å›¾åƒ:\n"
        for i, (img_name, imp) in enumerate(improvements[:5], 1):
            report += f"  {i}. {img_name}: {imp*100:+.2f}%\n"
        
        report += "\nä¸‹é™æœ€å¤§çš„5å¼ å›¾åƒ:\n"
        for i, (img_name, imp) in enumerate(improvements[-5:], 1):
            report += f"  {i}. {img_name}: {imp*100:+.2f}%\n"
        
        report += f"\n{'='*60}\n"
        
        return report


def main():
    parser = argparse.ArgumentParser(description="ä¸¤é˜¶æ®µçº§è”æ£€æµ‹ - æ‰¹é‡è¯„ä¼°")
    parser.add_argument('--yolo-model', type=str, required=True, help='YOLOæ¨¡å‹è·¯å¾„')
    parser.add_argument('--classifier', type=str, required=True, help='åˆ†ç±»å™¨æƒé‡è·¯å¾„')
    parser.add_argument('--data-yaml', type=str, required=True, help='æ•°æ®é›†YAMLé…ç½®')
    parser.add_argument('--split', type=str, default='val', choices=['train', 'val'], 
                       help='æ•°æ®é›†åˆ’åˆ†')
    parser.add_argument('--model-type', type=str, default='mobilenet',
                       choices=['mlp', 'mobilenet'], help='åˆ†ç±»å™¨ç±»å‹')
    parser.add_argument('--input-size', type=int, default=112, help='åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸')
    parser.add_argument('--num-classes', type=int, default=2, help='ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰')
    parser.add_argument('--imgsz', type=int, default=1280, help='æ¨ç†å°ºå¯¸')
    
    # é˜ˆå€¼å‚æ•°
    parser.add_argument('--stage1-conf', type=float, default=0.05, 
                       help='ç¬¬ä¸€é˜¶æ®µç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--stage2-threshold', type=float, default=0.5,
                       help='ç¬¬äºŒé˜¶æ®µåˆ†ç±»é˜ˆå€¼')
    parser.add_argument('--yolo-conf', type=float, default=0.25,
                       help='å•é˜¶æ®µYOLOç½®ä¿¡åº¦ï¼ˆç”¨äºå¯¹æ¯”ï¼‰')
    
    # NMSå‚æ•°
    parser.add_argument('--cross-class-nms', action='store_true', default=True,
                       help='å¯ç”¨è·¨ç±»åˆ«NMS')
    parser.add_argument('--no-cross-class-nms', action='store_false', dest='cross_class_nms',
                       help='ç¦ç”¨è·¨ç±»åˆ«NMS')
    parser.add_argument('--nms-iou', type=float, default=0.3,
                       help='è·¨ç±»åˆ«NMSçš„IOUé˜ˆå€¼')
    
    parser.add_argument('--save-dir', type=str, default='runs/cascaded_eval',
                       help='ä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CascadedEvaluator(
        args.yolo_model,
        args.classifier,
        classifier_type=args.model_type,
        input_size=args.input_size,
        num_classes=args.num_classes,
        device=args.device,
        cross_class_nms=args.cross_class_nms,
        nms_iou=args.nms_iou
    )
    
    # è¿è¡Œè¯„ä¼°
    evaluator.evaluate_dataset(
        args.data_yaml,
        split=args.split,
        stage1_conf=args.stage1_conf,
        stage2_threshold=args.stage2_threshold,
        yolo_conf=args.yolo_conf,
        imgsz=args.imgsz,
        save_dir=args.save_dir
    )


if __name__ == '__main__':
    main()

