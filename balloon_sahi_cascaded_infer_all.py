#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºSAHIçš„ä¸¤é˜¶æ®µçº§è”æ£€æµ‹ - æ‰¹é‡æ¨ç†

åŠŸèƒ½ï¼š
1. ä½¿ç”¨SAHIå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œåˆ‡ç‰‡æ¨ç†
2. å¯¹SAHIçš„æ¯ä¸ªæ£€æµ‹æ¡†ä½¿ç”¨MobileNetV2è¿›è¡ŒäºŒæ¬¡åˆ†ç±»
3. ä¿å­˜ç²¾ä¿®åçš„ç»“æœï¼ˆimages + labels + å¯è§†åŒ–ï¼‰
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
"""

import os
import sys
import cv2
import yaml
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import argparse
from tqdm import tqdm
from PIL import Image
import torchvision.transforms as transforms
from collections import defaultdict

# SAHI imports
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction

# å¯¼å…¥åˆ†ç±»å™¨æ¨¡å‹
sys.path.insert(0, str(Path(__file__).parent))
from balloon_cascaded_detection import MobileNetClassifier, SimpleMLP, parse_yolo_label


class SAHICascadedDetector:
    """åŸºäºSAHIçš„ä¸¤é˜¶æ®µçº§è”æ£€æµ‹å™¨"""
    
    def __init__(self, yolo_model_path: str, classifier_path: str,
                 classifier_type: str = "mobilenet", input_size: int = 112,
                 num_classes: int = 2, device: str = "cuda:0",
                 # SAHIå‚æ•°
                 slice_height: int = 640, slice_width: int = 640,
                 overlap_ratio: float = 0.2, conf_threshold: float = 0.25,
                 # äºŒé˜¶æ®µå‚æ•°
                 stage2_threshold: float = 0.5):
        """
        åˆå§‹åŒ–SAHIä¸¤é˜¶æ®µæ£€æµ‹å™¨
        
        Args:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            classifier_path: åˆ†ç±»å™¨æƒé‡è·¯å¾„
            classifier_type: åˆ†ç±»å™¨ç±»å‹ ('mlp' æˆ– 'mobilenet')
            input_size: åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸
            num_classes: ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
            device: è®¾å¤‡
            slice_height: SAHIåˆ‡ç‰‡é«˜åº¦
            slice_width: SAHIåˆ‡ç‰‡å®½åº¦
            overlap_ratio: SAHIé‡å æ¯”ä¾‹
            conf_threshold: SAHIç½®ä¿¡åº¦é˜ˆå€¼
            stage2_threshold: äºŒé˜¶æ®µåˆ†ç±»é˜ˆå€¼
        """
        self.device = device
        self.stage2_threshold = stage2_threshold
        self.input_size = input_size
        
        # åŠ è½½SAHIæ£€æµ‹æ¨¡å‹
        print(f"âœ… åŠ è½½SAHIæ£€æµ‹æ¨¡å‹...")
        self.detection_model = AutoDetectionModel.from_pretrained(
            model_type='yolov8',
            model_path=yolo_model_path,
            confidence_threshold=conf_threshold,
            device=device
        )
        
        # SAHIå‚æ•°
        self.slice_height = slice_height
        self.slice_width = slice_width
        self.overlap_ratio = overlap_ratio
        
        print(f"   YOLOæ¨¡å‹: {yolo_model_path}")
        print(f"   åˆ‡ç‰‡å°ºå¯¸: {slice_height}x{slice_width}")
        print(f"   é‡å æ¯”ä¾‹: {overlap_ratio}")
        print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
        
        # åŠ è½½äºŒé˜¶æ®µåˆ†ç±»å™¨
        print(f"âœ… åŠ è½½äºŒé˜¶æ®µåˆ†ç±»å™¨...")
        if classifier_type == "mlp":
            self.classifier = SimpleMLP(input_size, num_classes)
        else:
            self.classifier = MobileNetClassifier(num_classes)
        
        self.classifier.load_state_dict(torch.load(classifier_path, map_location=device))
        self.classifier.to(device)
        self.classifier.eval()
        
        print(f"   åˆ†ç±»å™¨: {classifier_path}")
        print(f"   ç±»å‹: {classifier_type}")
        print(f"   é˜ˆå€¼: {stage2_threshold}")
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((input_size, input_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def detect_with_sahi(self, image_path: str) -> Tuple[List[Dict], List[Dict]]:
        """
        ä½¿ç”¨SAHIè¿›è¡Œåˆ‡ç‰‡æ¨ç†ï¼Œç„¶åä½¿ç”¨äºŒé˜¶æ®µåˆ†ç±»å™¨ç²¾ä¿®
        
        Args:
            image_path: å›¾åƒè·¯å¾„
        
        Returns:
            (stage1_detections, stage2_detections)
            stage1_detections: SAHIåŸå§‹æ£€æµ‹ç»“æœ
            stage2_detections: äºŒé˜¶æ®µç²¾ä¿®åçš„ç»“æœ
        """
        # è¯»å–å›¾åƒ
        img = cv2.imread(image_path)
        img_h, img_w = img.shape[:2]
        
        # SAHIåˆ‡ç‰‡æ¨ç†
        result = get_sliced_prediction(
            image_path,
            self.detection_model,
            slice_height=self.slice_height,
            slice_width=self.slice_width,
            overlap_height_ratio=self.overlap_ratio,
            overlap_width_ratio=self.overlap_ratio,
            postprocess_type="NMS",
            postprocess_match_metric="IOS",
            postprocess_match_threshold=0.5
        )
        
        # æå–SAHIæ£€æµ‹ç»“æœ
        stage1_detections = []
        for obj in result.object_prediction_list:
            bbox = obj.bbox
            stage1_detections.append({
                'box': [bbox.minx, bbox.miny, bbox.maxx, bbox.maxy],
                'cls': obj.category.id,
                'conf': obj.score.value
            })
        
        # äºŒé˜¶æ®µåˆ†ç±»
        stage2_detections = []
        for det in stage1_detections:
            x1, y1, x2, y2 = det['box']
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            
            # è¾¹ç•Œæ£€æŸ¥
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(img_w, x2)
            y2 = min(img_h, y2)
            
            if x2 <= x1 or y2 <= y1:
                continue
            
            # è£å‰ªå€™é€‰åŒºåŸŸ
            crop = img[y1:y2, x1:x2]
            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
            
            # åˆ†ç±»å™¨æ¨ç†
            crop_tensor = self.transform(crop_pil).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                output = self.classifier(crop_tensor)
                probs = F.softmax(output, dim=1)
                stage2_conf, stage2_cls = probs.max(1)
                stage2_conf = stage2_conf.item()
                stage2_cls = stage2_cls.item()
            
            # è¿‡æ»¤ï¼šå¦‚æœåˆ†ç±»å™¨åˆ¤æ–­ä¸ºèƒŒæ™¯ï¼ˆcls=0ï¼‰æˆ–ç½®ä¿¡åº¦ä½ï¼Œåˆ™ä¸¢å¼ƒ
            if stage2_cls == 0 or stage2_conf < self.stage2_threshold:
                continue
            
            # è½¬æ¢ç±»åˆ«ï¼ˆåˆ†ç±»å™¨çš„ç±»åˆ«ä»1å¼€å§‹ï¼Œéœ€è¦å‡1ï¼‰
            final_cls = stage2_cls - 1
            
            stage2_detections.append({
                'box': [float(x1), float(y1), float(x2), float(y2)],
                'cls': final_cls,
                'conf': stage2_conf,
                'stage1_cls': det['cls'],
                'stage1_conf': det['conf']
            })
        
        return stage1_detections, stage2_detections
    
    def save_yolo_label(self, detections: List[Dict], save_path: str, img_w: int, img_h: int):
        """ä¿å­˜ä¸ºYOLOæ ¼å¼æ ‡ç­¾"""
        with open(save_path, 'w') as f:
            for det in detections:
                x1, y1, x2, y2 = det['box']
                x_center = (x1 + x2) / 2 / img_w
                y_center = (y1 + y2) / 2 / img_h
                w = (x2 - x1) / img_w
                h = (y2 - y1) / img_h
                
                f.write(f"{det['cls']} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\n")
    
    def draw_detections(self, img: np.ndarray, detections: List[Dict], 
                       color: Tuple[int, int, int] = (0, 255, 0)) -> np.ndarray:
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹æ¡†"""
        img_draw = img.copy()
        for det in detections:
            x1, y1, x2, y2 = [int(v) for v in det['box']]
            cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, 2)
            label = f"cls{det['cls']} {det['conf']:.2f}"
            cv2.putText(img_draw, label, (x1, y1-5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        return img_draw
    
    def visualize_comparison(self, img: np.ndarray, stage1_dets: List[Dict],
                            stage2_dets: List[Dict], save_path: str):
        """å¯è§†åŒ–SAHI vs äºŒé˜¶æ®µå¯¹æ¯”"""
        img_h, img_w = img.shape[:2]
        
        # ç»˜åˆ¶SAHIç»“æœ
        img_sahi = self.draw_detections(img, stage1_dets, color=(0, 0, 255))  # çº¢è‰²
        
        # ç»˜åˆ¶äºŒé˜¶æ®µç»“æœ
        img_stage2 = self.draw_detections(img, stage2_dets, color=(0, 255, 0))  # ç»¿è‰²
        
        # æ‹¼æ¥ä¸¤å¼ å›¾åƒ
        gap = np.ones((img_h, 20, 3), dtype=np.uint8) * 255
        vis_img = np.hstack([img_sahi, gap, img_stage2])
        
        # æ·»åŠ æ ‡é¢˜
        title_height = 50
        title_bar = np.ones((title_height, vis_img.shape[1], 3), dtype=np.uint8) * 255
        cv2.putText(title_bar, f"SAHI ({len(stage1_dets)} dets)", 
                   (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
        cv2.putText(title_bar, f"SAHI + Stage2 ({len(stage2_dets)} dets)", 
                   (img_w + 30, 35), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
        
        vis_img = np.vstack([title_bar, vis_img])
        
        # ä¿å­˜
        cv2.imwrite(save_path, vis_img)
    
    def count_objects(self, detections: List[Dict]) -> Dict[int, int]:
        """ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡"""
        counts = defaultdict(int)
        for det in detections:
            counts[det['cls']] += 1
        return dict(counts)
    
    def count_gt_objects(self, label_path: str, img_w: int, img_h: int) -> Dict[int, int]:
        """ç»Ÿè®¡GTä¸­æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡"""
        gt_boxes = parse_yolo_label(label_path, img_w, img_h)
        counts = defaultdict(int)
        for box in gt_boxes:
            counts[int(box[0])] += 1
        return dict(counts)
    
    def calculate_count_accuracy(self, pred_counts: Dict[int, int], 
                                 gt_counts: Dict[int, int]) -> float:
        """è®¡ç®—æ•°é‡å‡†ç¡®ç‡: 1 - |predict-true|/true"""
        all_classes = set(list(pred_counts.keys()) + list(gt_counts.keys()))
        
        total_error = 0
        total_gt = 0
        
        for cls in all_classes:
            pred = pred_counts.get(cls, 0)
            gt = gt_counts.get(cls, 0)
            
            if gt > 0:
                error = abs(pred - gt) / gt
                total_error += error * gt
                total_gt += gt
        
        if total_gt == 0:
            return 1.0
        
        accuracy = 1.0 - (total_error / total_gt)
        return max(0.0, accuracy)


def evaluate_dataset(yolo_model: str, classifier: str, data_yaml: str,
                     split: str = "val", save_dir: str = "runs/sahi_cascaded_eval",
                     classifier_type: str = "mobilenet", input_size: int = 112,
                     num_classes: int = 2, device: str = "cuda:0",
                     # SAHIå‚æ•°
                     slice_height: int = 640, slice_width: int = 640,
                     overlap_ratio: float = 0.2, sahi_conf: float = 0.25,
                     # äºŒé˜¶æ®µå‚æ•°
                     stage2_threshold: float = 0.5):
    """
    åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡ŒSAHIä¸¤é˜¶æ®µæ‰¹é‡æ¨ç†å’Œè¯„ä¼°
    
    Args:
        yolo_model: YOLOæ¨¡å‹è·¯å¾„
        classifier: åˆ†ç±»å™¨æƒé‡è·¯å¾„
        data_yaml: æ•°æ®é›†YAMLé…ç½®
        split: 'train' æˆ– 'val'
        save_dir: ä¿å­˜ç›®å½•
        å…¶ä»–å‚æ•°è§SAHICascadedDetector
    """
    # è¯»å–æ•°æ®é›†é…ç½®
    with open(data_yaml, 'r') as f:
        data_config = yaml.safe_load(f)
    
    dataset_path = Path(data_config['path'])
    # æ”¯æŒä¸¤ç§ç›®å½•ç»“æ„
    if (dataset_path / 'images' / split).exists():
        image_dir = dataset_path / 'images' / split
        label_dir = dataset_path / 'labels' / split
    else:
        image_dir = dataset_path / data_config[split] / 'images'
        label_dir = dataset_path / data_config[split] / 'labels'
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_path = Path(save_dir)
    images_dir = save_path / 'images'
    labels_dir_sahi = save_path / 'labels_sahi'
    labels_dir_stage2 = save_path / 'labels_sahi_stage2'
    vis_comp_dir = save_path / 'visualizations_comparison'
    
    images_dir.mkdir(parents=True, exist_ok=True)
    labels_dir_sahi.mkdir(exist_ok=True)
    labels_dir_stage2.mkdir(exist_ok=True)
    vis_comp_dir.mkdir(exist_ok=True)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = SAHICascadedDetector(
        yolo_model, classifier, classifier_type, input_size,
        num_classes, device, slice_height, slice_width,
        overlap_ratio, sahi_conf, stage2_threshold
    )
    
    # è·å–æ‰€æœ‰å›¾åƒ
    image_files = sorted(image_dir.glob("*.jpg")) + sorted(image_dir.glob("*.png"))
    
    print(f"\nğŸ” SAHIä¸¤é˜¶æ®µæ‰¹é‡æ¨ç† ({len(image_files)} å¼ å›¾åƒ)...")
    print(f"   æ•°æ®é›†: {data_yaml}")
    print(f"   åˆ’åˆ†: {split}")
    print(f"   ä¿å­˜ç›®å½•: {save_dir}")
    
    # ç»“æœå­˜å‚¨
    results = {
        'sahi': [],
        'sahi_stage2': [],
        'comparison': []
    }
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'sahi': {
            'count_accuracies': [],
            'total_detections': 0,
            'total_gt': 0
        },
        'sahi_stage2': {
            'count_accuracies': [],
            'total_detections': 0,
            'total_gt': 0
        }
    }
    
    # å¤„ç†æ¯å¼ å›¾åƒ
    for img_path in tqdm(image_files, desc="æ¨ç†ä¸­"):
        # è¯»å–å›¾åƒ
        img = cv2.imread(str(img_path))
        if img is None:
            continue
        
        img_h, img_w = img.shape[:2]
        
        # è¯»å–GT
        label_path = label_dir / (img_path.stem + '.txt')
        gt_counts = detector.count_gt_objects(str(label_path), img_w, img_h)
        total_gt = sum(gt_counts.values())
        
        # SAHIä¸¤é˜¶æ®µæ£€æµ‹
        stage1_dets, stage2_dets = detector.detect_with_sahi(str(img_path))
        
        # è®¡ç®—å‡†ç¡®ç‡
        sahi_counts = detector.count_objects(stage1_dets)
        sahi_acc = detector.calculate_count_accuracy(sahi_counts, gt_counts)
        
        stage2_counts = detector.count_objects(stage2_dets)
        stage2_acc = detector.calculate_count_accuracy(stage2_counts, gt_counts)
        
        # è®°å½•ç»“æœ
        results['sahi'].append({
            'image': img_path.name,
            'gt_counts': gt_counts,
            'pred_counts': sahi_counts,
            'count_accuracy': sahi_acc,
            'num_detections': len(stage1_dets)
        })
        
        results['sahi_stage2'].append({
            'image': img_path.name,
            'gt_counts': gt_counts,
            'pred_counts': stage2_counts,
            'count_accuracy': stage2_acc,
            'num_detections': len(stage2_dets)
        })
        
        results['comparison'].append({
            'image': img_path.name,
            'gt_total': total_gt,
            'sahi': {
                'count': sum(sahi_counts.values()),
                'accuracy': sahi_acc
            },
            'sahi_stage2': {
                'count': sum(stage2_counts.values()),
                'accuracy': stage2_acc
            },
            'improvement': stage2_acc - sahi_acc
        })
        
        # æ›´æ–°ç»Ÿè®¡
        stats['sahi']['count_accuracies'].append(sahi_acc)
        stats['sahi']['total_detections'] += len(stage1_dets)
        stats['sahi']['total_gt'] += total_gt
        
        stats['sahi_stage2']['count_accuracies'].append(stage2_acc)
        stats['sahi_stage2']['total_detections'] += len(stage2_dets)
        stats['sahi_stage2']['total_gt'] += total_gt
        
        # ä¿å­˜æ¨ç†å›¾åƒï¼ˆäºŒé˜¶æ®µç»“æœï¼‰
        img_stage2 = detector.draw_detections(img, stage2_dets)
        cv2.imwrite(str(images_dir / img_path.name), img_stage2)
        
        # ä¿å­˜labels
        detector.save_yolo_label(stage1_dets, str(labels_dir_sahi / f"{img_path.stem}.txt"), img_w, img_h)
        detector.save_yolo_label(stage2_dets, str(labels_dir_stage2 / f"{img_path.stem}.txt"), img_w, img_h)
        
        # ä¿å­˜å¯è§†åŒ–å¯¹æ¯”
        detector.visualize_comparison(img, stage1_dets, stage2_dets,
                                      str(vis_comp_dir / f"{img_path.stem}_comparison.jpg"))
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    sahi_avg_acc = np.mean(stats['sahi']['count_accuracies']) * 100
    stage2_avg_acc = np.mean(stats['sahi_stage2']['count_accuracies']) * 100
    improvement = stage2_avg_acc - sahi_avg_acc
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(save_path / 'detailed_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""
{'='*60}
SAHIä¸¤é˜¶æ®µçº§è”æ£€æµ‹ - è¯„ä¼°æŠ¥å‘Š
{'='*60}

ã€æ€»ä½“æŒ‡æ ‡ã€‘

SAHIåˆ‡ç‰‡æ¨ç†:
  - å¹³å‡æ•°é‡å‡†ç¡®ç‡: {sahi_avg_acc:.2f}%
  - æ€»æ£€æµ‹æ•°: {stats['sahi']['total_detections']}
  - æ€»GTæ•°: {stats['sahi']['total_gt']}

SAHI + äºŒé˜¶æ®µåˆ†ç±»:
  - å¹³å‡æ•°é‡å‡†ç¡®ç‡: {stage2_avg_acc:.2f}%
  - æ€»æ£€æµ‹æ•°: {stats['sahi_stage2']['total_detections']}
  - æ€»GTæ•°: {stats['sahi_stage2']['total_gt']}

æ€§èƒ½æå‡:
  - å‡†ç¡®ç‡æå‡: {improvement:+.2f}%
  - {'âœ… äºŒé˜¶æ®µæ›´ä¼˜' if improvement > 0 else 'âš ï¸ äºŒé˜¶æ®µæœªæå‡'}

{'='*60}

ã€è¯¦ç»†åˆ†æã€‘

æå‡æœ€å¤§çš„å‰5å¼ å›¾åƒ:
"""
    
    # æ’åºæ‰¾å‡ºæå‡æœ€å¤§çš„å›¾åƒ
    sorted_results = sorted(results['comparison'], key=lambda x: x['improvement'], reverse=True)
    for i, item in enumerate(sorted_results[:5], 1):
        report += f"  {i}. {item['image']}: {item['improvement']*100:+.2f}%\n"
    
    report += "\nä¸‹é™æœ€å¤§çš„5å¼ å›¾åƒ:\n"
    for i, item in enumerate(sorted_results[-5:], 1):
        report += f"  {i}. {item['image']}: {item['improvement']*100:+.2f}%\n"
    
    report += f"\n{'='*60}\n"
    
    # ä¿å­˜å’Œæ‰“å°æŠ¥å‘Š
    with open(save_path / 'evaluation_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(report)
    print(f"\nâœ… è¯„ä¼°å®Œæˆ! ç»“æœä¿å­˜è‡³: {save_path}")
    print(f"   æ¨ç†å›¾åƒ: {images_dir}")
    print(f"   SAHIæ ‡ç­¾: {labels_dir_sahi}")
    print(f"   äºŒé˜¶æ®µæ ‡ç­¾: {labels_dir_stage2}")
    print(f"   å¯è§†åŒ–å¯¹æ¯”: {vis_comp_dir}")


def main():
    parser = argparse.ArgumentParser(description="åŸºäºSAHIçš„ä¸¤é˜¶æ®µçº§è”æ£€æµ‹ - æ‰¹é‡æ¨ç†")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--yolo-model', type=str, required=True, help='YOLOæ¨¡å‹è·¯å¾„')
    parser.add_argument('--classifier', type=str, required=True, help='åˆ†ç±»å™¨æƒé‡è·¯å¾„')
    parser.add_argument('--data-yaml', type=str, required=True, help='æ•°æ®é›†YAMLé…ç½®')
    parser.add_argument('--split', type=str, default='val', choices=['train', 'val'], 
                       help='æ•°æ®é›†åˆ’åˆ†')
    
    # åˆ†ç±»å™¨å‚æ•°
    parser.add_argument('--model-type', type=str, default='mobilenet',
                       choices=['mlp', 'mobilenet'], help='åˆ†ç±»å™¨ç±»å‹')
    parser.add_argument('--input-size', type=int, default=112, help='åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸')
    parser.add_argument('--num-classes', type=int, default=2, help='ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰')
    
    # SAHIå‚æ•°
    parser.add_argument('--slice-height', type=int, default=640, help='SAHIåˆ‡ç‰‡é«˜åº¦')
    parser.add_argument('--slice-width', type=int, default=640, help='SAHIåˆ‡ç‰‡å®½åº¦')
    parser.add_argument('--overlap-ratio', type=float, default=0.2, help='SAHIé‡å æ¯”ä¾‹')
    parser.add_argument('--sahi-conf', type=float, default=0.25, help='SAHIç½®ä¿¡åº¦é˜ˆå€¼')
    
    # äºŒé˜¶æ®µå‚æ•°
    parser.add_argument('--stage2-threshold', type=float, default=0.5, help='äºŒé˜¶æ®µåˆ†ç±»é˜ˆå€¼')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--save-dir', type=str, default='runs/sahi_cascaded_eval',
                       help='ä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    # è¿è¡Œè¯„ä¼°
    evaluate_dataset(
        yolo_model=args.yolo_model,
        classifier=args.classifier,
        data_yaml=args.data_yaml,
        split=args.split,
        save_dir=args.save_dir,
        classifier_type=args.model_type,
        input_size=args.input_size,
        num_classes=args.num_classes,
        device=args.device,
        slice_height=args.slice_height,
        slice_width=args.slice_width,
        overlap_ratio=args.overlap_ratio,
        sahi_conf=args.sahi_conf,
        stage2_threshold=args.stage2_threshold
    )


if __name__ == '__main__':
    main()

