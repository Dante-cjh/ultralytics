#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸¤é˜¶æ®µçº§è”æ£€æµ‹ç³»ç»Ÿ - Balloonæ•°æ®é›†ç‰ˆæœ¬

ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨YOLOæ¨¡å‹ç”Ÿæˆå€™é€‰æ¡†ï¼ˆä½ç½®ä¿¡åº¦ï¼‰
ç¬¬äºŒé˜¶æ®µï¼šå¯¹å€™é€‰æ¡†è¿›è¡Œé‡åˆ†ç±»ï¼ˆä½¿ç”¨è½»é‡çº§åˆ†ç±»å™¨ï¼‰

ä½¿ç”¨æ–¹æ³•:
1. å‡†å¤‡æ•°æ®: python balloon_cascaded_detection.py prepare --yolo-model <path> --conf 0.05
2. è®­ç»ƒåˆ†ç±»å™¨: python balloon_cascaded_detection.py train --data-dir <path>
3. æ¨ç†: python balloon_cascaded_detection.py infer --yolo-model <path> --classifier <path> --image <path>
"""

import os
import cv2
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
from typing import List, Tuple, Dict
import argparse
from tqdm import tqdm
from ultralytics import YOLO
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image


# ==================== å·¥å…·å‡½æ•° ====================

def calculate_iou(box1: np.ndarray, box2: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ¡†çš„IOU
    
    Args:
        box1, box2: [x1, y1, x2, y2]
    
    Returns:
        IOUå€¼
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0


def assign_labels(proposals: List[Tuple], gt_boxes: List[Tuple], iou_threshold: float = 0.5) -> List[Dict]:
    """
    ä¸ºå€™é€‰æ¡†åˆ†é…æ ‡ç­¾ï¼ˆç±»ä¼¼äºFaster R-CNNçš„assignerï¼‰
    
    Args:
        proposals: å€™é€‰æ¡†åˆ—è¡¨ [(cls, conf, x1, y1, x2, y2), ...]
        gt_boxes: GTæ¡†åˆ—è¡¨ [(cls, x1, y1, x2, y2), ...]
        iou_threshold: IOUé˜ˆå€¼
    
    Returns:
        æ ‡æ³¨åçš„å€™é€‰æ¡† [{'box': [x1,y1,x2,y2], 'pred_cls': int, 'true_cls': int, 'is_positive': bool}, ...]
        - is_positive=True: æ­£æ ·æœ¬ï¼ˆä¸GTåŒ¹é…ï¼‰
        - is_positive=False: è´Ÿæ ·æœ¬ï¼ˆèƒŒæ™¯ï¼‰
        - true_cls: GTç±»åˆ«ï¼Œ-1è¡¨ç¤ºèƒŒæ™¯
    """
    labeled_proposals = []
    
    for proposal in proposals:
        if len(proposal) == 6:
            pred_cls, conf, x1, y1, x2, y2 = proposal
        else:
            pred_cls, x1, y1, x2, y2 = proposal[:5]
            conf = 1.0
        
        prop_box = np.array([x1, y1, x2, y2])
        
        # å¯»æ‰¾æœ€ä½³åŒ¹é…çš„GT
        max_iou = 0.0
        matched_gt_cls = -1  # -1è¡¨ç¤ºèƒŒæ™¯
        
        for gt in gt_boxes:
            gt_cls = int(gt[0])
            gt_box = np.array([gt[1], gt[2], gt[3], gt[4]])
            
            iou = calculate_iou(prop_box, gt_box)
            if iou > max_iou:
                max_iou = iou
                matched_gt_cls = gt_cls
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ­£æ ·æœ¬
        is_positive = max_iou >= iou_threshold
        
        labeled_proposals.append({
            'box': [float(x1), float(y1), float(x2), float(y2)],
            'pred_cls': int(pred_cls),
            'conf': float(conf),
            'true_cls': matched_gt_cls if is_positive else -1,  # -1è¡¨ç¤ºèƒŒæ™¯
            'is_positive': is_positive,
            'iou': float(max_iou)
        })
    
    return labeled_proposals


def parse_yolo_label(label_path: str, img_w: int, img_h: int) -> List[Tuple]:
    """è§£æYOLOæ ¼å¼æ ‡ç­¾æ–‡ä»¶"""
    boxes = []
    
    if not os.path.exists(label_path):
        return boxes
    
    with open(label_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 5:
                cls = int(parts[0])
                x_center = float(parts[1]) * img_w
                y_center = float(parts[2]) * img_h
                w = float(parts[3]) * img_w
                h = float(parts[4]) * img_h
                
                x1 = x_center - w / 2
                y1 = y_center - h / 2
                x2 = x_center + w / 2
                y2 = y_center + h / 2
                
                boxes.append((cls, x1, y1, x2, y2))
    
    return boxes


def cross_class_nms(detections: List[Dict], iou_threshold: float = 0.3) -> List[Dict]:
    """
    è·¨ç±»åˆ«NMSï¼šå¤„ç†ä¸åŒç±»åˆ«é¢„æµ‹åŒä¸€ç›®æ ‡çš„æƒ…å†µ
    
    ç­–ç•¥ï¼šå¯¹äºé«˜åº¦é‡å çš„æ¡†ï¼ˆå³ä½¿ç±»åˆ«ä¸åŒï¼‰ï¼Œåªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
    
    Args:
        detections: æ£€æµ‹ç»“æœåˆ—è¡¨ [{'box': [x1,y1,x2,y2], 'cls': int, 'conf': float}, ...]
        iou_threshold: IOUé˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„æ¡†ä¼šè¢«æŠ‘åˆ¶
    
    Returns:
        NMSåçš„æ£€æµ‹ç»“æœ
    """
    if len(detections) == 0:
        return detections
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    boxes = np.array([d['box'] for d in detections])
    scores = np.array([d['conf'] for d in detections])
    
    # è®¡ç®—æ‰€æœ‰æ¡†çš„é¢ç§¯
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    areas = (x2 - x1) * (y2 - y1)
    
    # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
    order = scores.argsort()[::-1]
    
    keep = []
    while order.size > 0:
        # ä¿ç•™å½“å‰ç½®ä¿¡åº¦æœ€é«˜çš„æ¡†
        i = order[0]
        keep.append(i)
        
        if order.size == 1:
            break
        
        # è®¡ç®—å½“å‰æ¡†ä¸å…¶ä»–æ¡†çš„IOU
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        
        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h
        
        ovr = inter / (areas[i] + areas[order[1:]] - inter)
        
        # ä¿ç•™IOUå°äºé˜ˆå€¼çš„æ¡†
        inds = np.where(ovr <= iou_threshold)[0]
        order = order[inds + 1]
    
    return [detections[i] for i in keep]


# ==================== æ•°æ®å‡†å¤‡ ====================

class CascadedDataPreparer:
    """ç¬¬ä¸€é˜¶æ®µæ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self, yolo_model_path: str, conf_threshold: float = 0.05, 
                 iou_threshold: float = 0.5, device: str = "cuda:0"):
        """
        åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨
        
        Args:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            conf_threshold: ç¬¬ä¸€é˜¶æ®µç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä½é˜ˆå€¼ä»¥è·å¾—æ›´å¤šå€™é€‰æ¡†ï¼‰
            iou_threshold: ä¸GTåŒ¹é…çš„IOUé˜ˆå€¼
            device: è®¾å¤‡
        """
        self.model = YOLO(yolo_model_path)
        self.model.to(device)
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.device = device
        print(f"âœ… åŠ è½½YOLOæ¨¡å‹: {yolo_model_path}")
        print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}, IOUé˜ˆå€¼: {iou_threshold}")
    
    def generate_proposals(self, image_path: str, imgsz: int = 1280) -> List[Tuple]:
        """
        ä½¿ç”¨YOLOç”Ÿæˆå€™é€‰æ¡†
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            imgsz: æ¨ç†å°ºå¯¸
        
        Returns:
            å€™é€‰æ¡†åˆ—è¡¨ [(cls, conf, x1, y1, x2, y2), ...]
        """
        results = self.model.predict(
            source=image_path,
            imgsz=imgsz,
            conf=self.conf_threshold,
            iou=0.45,  # NMSé˜ˆå€¼
            device=self.device,
            verbose=False,
            save=False,
        )
        
        result = results[0]
        proposals = []
        
        if len(result.boxes) > 0:
            xyxy = result.boxes.xyxy.cpu().numpy()
            cls = result.boxes.cls.cpu().numpy()
            conf = result.boxes.conf.cpu().numpy()
            
            for i in range(len(xyxy)):
                proposals.append((
                    int(cls[i]),
                    float(conf[i]),
                    float(xyxy[i][0]),
                    float(xyxy[i][1]),
                    float(xyxy[i][2]),
                    float(xyxy[i][3])
                ))
        
        return proposals
    
    def prepare_dataset(self, data_yaml: str, split: str = "train", 
                       output_dir: str = "cascaded_data", imgsz: int = 1280,
                       force: bool = False, negative_ratio: float = 2.0,
                       balance_samples: bool = True):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›†
        
        Args:
            data_yaml: æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶
            split: 'train' æˆ– 'val'
            output_dir: è¾“å‡ºç›®å½•
            imgsz: æ¨ç†å°ºå¯¸
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼ˆå¦‚æœä¸ºFalseä¸”æ•°æ®å·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡ï¼‰
            negative_ratio: è´Ÿæ ·æœ¬ä¸æ­£æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆé»˜è®¤2.0ï¼Œå³è´Ÿæ ·æœ¬æ•°=æ­£æ ·æœ¬æ•°*2ï¼‰
            balance_samples: æ˜¯å¦å¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼ˆä¸‹é‡‡æ ·å¤šæ•°ç±»ï¼‰
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir) / split
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ•°æ®
        if not force and output_path.exists():
            list_file = output_path / 'data_list.json'
            stats_file = output_path / 'stats.json'
            
            if list_file.exists() and stats_file.exists():
                print(f"\nâ­ï¸  {split}é›†æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡å‡†å¤‡æ­¥éª¤")
                print(f"   æ•°æ®è·¯å¾„: {output_path}")
                print(f"   å¦‚éœ€é‡æ–°ç”Ÿæˆï¼Œè¯·ä½¿ç”¨ --force å‚æ•°")
                
                # è¯»å–å¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                import json
                with open(stats_file, 'r') as f:
                    stats = json.load(f)
                print(f"   æ€»å›¾åƒæ•°: {stats['total_images']}")
                print(f"   æ€»å€™é€‰æ¡†æ•°: {stats['total_proposals']}")
                print(f"   æ­£æ ·æœ¬æ•°: {stats['positive_samples']}")
                print(f"   è´Ÿæ ·æœ¬æ•°: {stats['negative_samples']}")
                return
        # è¯»å–æ•°æ®é›†é…ç½®
        with open(data_yaml, 'r') as f:
            data_config = yaml.safe_load(f)
        
        dataset_path = Path(data_config['path'])
        # æ”¯æŒä¸¤ç§ç›®å½•ç»“æ„ï¼š
        # 1. path/train/images å’Œ path/train/labels
        # 2. path/images/train å’Œ path/labels/train (balloonæ ¼å¼)
        if (dataset_path / 'images' / split).exists():
            # Balloonæ ¼å¼
            image_dir = dataset_path / 'images' / split
            label_dir = dataset_path / 'labels' / split
        else:
            # æ ‡å‡†æ ¼å¼
            image_dir = dataset_path / data_config[split] / 'images'
            label_dir = dataset_path / data_config[split] / 'labels'
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir) / split
        crops_dir = output_path / 'crops'
        crops_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_images': 0,
            'total_proposals': 0,
            'positive_samples': 0,
            'negative_samples': 0,
            'class_dist': {}
        }
        
        # å‡†å¤‡æ•°æ®åˆ—è¡¨
        data_list = []
        
        # å¤„ç†æ¯å¼ å›¾åƒ
        image_files = sorted(image_dir.glob("*.jpg")) + sorted(image_dir.glob("*.png"))
        
        print(f"\nğŸ” å¤„ç† {split} é›†...")
        for img_path in tqdm(image_files, desc=f"å‡†å¤‡{split}æ•°æ®"):
            # è¯»å–å›¾åƒ
            img = cv2.imread(str(img_path))
            if img is None:
                continue
            
            img_h, img_w = img.shape[:2]
            stats['total_images'] += 1
            
            # ç”Ÿæˆå€™é€‰æ¡†
            proposals = self.generate_proposals(str(img_path), imgsz)
            
            if len(proposals) == 0:
                continue
            
            # è¯»å–GT
            label_path = label_dir / (img_path.stem + '.txt')
            gt_boxes = parse_yolo_label(str(label_path), img_w, img_h)
            
            # åˆ†é…æ ‡ç­¾
            labeled_proposals = assign_labels(proposals, gt_boxes, self.iou_threshold)
            
            # ä¿å­˜æ¯ä¸ªå€™é€‰æ¡†
            for idx, prop in enumerate(labeled_proposals):
                x1, y1, x2, y2 = [int(v) for v in prop['box']]
                
                # è£å‰ªåŒºåŸŸï¼ˆå¸¦è¾¹ç•Œæ£€æŸ¥ï¼‰
                x1 = max(0, x1)
                y1 = max(0, y1)
                x2 = min(img_w, x2)
                y2 = min(img_h, y2)
                
                if x2 <= x1 or y2 <= y1:
                    continue
                
                crop = img[y1:y2, x1:x2]
                
                if crop.size == 0:
                    continue
                
                # ä¿å­˜è£å‰ªå›¾åƒ
                crop_name = f"{img_path.stem}_{idx}.jpg"
                crop_path = crops_dir / crop_name
                cv2.imwrite(str(crop_path), crop)
                
                # è®°å½•æ•°æ®ï¼ˆç¡®ä¿ç±»å‹å¯JSONåºåˆ—åŒ–ï¼‰
                data_list.append({
                    'crop_path': str(crop_path.relative_to(output_path)),
                    'true_cls': int(prop['true_cls']),
                    'pred_cls': int(prop['pred_cls']),
                    'conf': float(prop['conf']),
                    'iou': float(prop['iou']),
                    'is_positive': bool(prop['is_positive'])
                })
                
                # ç»Ÿè®¡
                stats['total_proposals'] += 1
                if prop['is_positive']:
                    stats['positive_samples'] += 1
                    cls = prop['true_cls']
                    stats['class_dist'][cls] = stats['class_dist'].get(cls, 0) + 1
                else:
                    stats['negative_samples'] += 1
        
        # æ ·æœ¬å¹³è¡¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if balance_samples and stats['positive_samples'] > 0 and stats['negative_samples'] > 0:
            print(f"\nâš–ï¸  æ­£è´Ÿæ ·æœ¬å¹³è¡¡...")
            print(f"   åŸå§‹ - æ­£æ ·æœ¬: {stats['positive_samples']}, è´Ÿæ ·æœ¬: {stats['negative_samples']}")
            print(f"   ç›®æ ‡è´Ÿæ ·æœ¬æ¯”ä¾‹: {negative_ratio}:1")
            
            target_negative = int(stats['positive_samples'] * negative_ratio)
            current_ratio = stats['negative_samples'] / stats['positive_samples']
            
            import random
            random.seed(42)  # å›ºå®šéšæœºç§å­ä»¥ä¿è¯å¯å¤ç°
            
            # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
            positive_samples = [s for s in data_list if s['is_positive']]
            negative_samples = [s for s in data_list if not s['is_positive']]
            
            if stats['negative_samples'] > target_negative:
                # æƒ…å†µ1: è´Ÿæ ·æœ¬è¿‡å¤š â†’ ä¸‹é‡‡æ ·è´Ÿæ ·æœ¬
                print(f"   ğŸ“‰ è´Ÿæ ·æœ¬è¿‡å¤šï¼Œä¸‹é‡‡æ ·è´Ÿæ ·æœ¬: {stats['negative_samples']} â†’ {target_negative}")
                
                # éšæœºé€‰æ‹©è´Ÿæ ·æœ¬
                sampled_negatives = random.sample(negative_samples, target_negative)
                
                # åˆå¹¶æ•°æ®
                data_list = positive_samples + sampled_negatives
                
                # æ›´æ–°ç»Ÿè®¡
                stats['negative_samples'] = target_negative
                stats['total_proposals'] = len(data_list)
                
                print(f"   âœ… å¹³è¡¡å - æ­£æ ·æœ¬: {stats['positive_samples']}, è´Ÿæ ·æœ¬: {stats['negative_samples']}")
                print(f"   è´Ÿæ ·æœ¬æ¯”ä¾‹: {stats['negative_samples']/stats['positive_samples']:.2f}:1")
            
            elif stats['negative_samples'] < target_negative:
                # æƒ…å†µ2: è´Ÿæ ·æœ¬è¿‡å°‘ï¼ˆæ­£æ ·æœ¬è¿‡å¤šï¼‰â†’ ä¸‹é‡‡æ ·æ­£æ ·æœ¬
                print(f"   ğŸ“‰ è´Ÿæ ·æœ¬ä¸è¶³ï¼Œä¸‹é‡‡æ ·æ­£æ ·æœ¬ä»¥è¾¾åˆ°å¹³è¡¡")
                print(f"   å½“å‰è´Ÿæ ·æœ¬æ¯”ä¾‹: {current_ratio:.2f}:1 (ç›®æ ‡: {negative_ratio}:1)")
                
                # è®¡ç®—ç›®æ ‡æ­£æ ·æœ¬æ•°
                target_positive = int(stats['negative_samples'] / negative_ratio)
                
                print(f"   ä¸‹é‡‡æ ·æ­£æ ·æœ¬: {stats['positive_samples']} â†’ {target_positive}")
                
                # éšæœºé€‰æ‹©æ­£æ ·æœ¬
                sampled_positives = random.sample(positive_samples, target_positive)
                
                # åˆå¹¶æ•°æ®
                data_list = sampled_positives + negative_samples
                
                # æ›´æ–°ç»Ÿè®¡
                stats['positive_samples'] = target_positive
                stats['total_proposals'] = len(data_list)
                
                print(f"   âœ… å¹³è¡¡å - æ­£æ ·æœ¬: {stats['positive_samples']}, è´Ÿæ ·æœ¬: {stats['negative_samples']}")
                print(f"   è´Ÿæ ·æœ¬æ¯”ä¾‹: {stats['negative_samples']/stats['positive_samples']:.2f}:1")
            
            else:
                # æƒ…å†µ3: è´Ÿæ ·æœ¬æ•°é‡å·²ç»åˆç† â†’ ä¸å¤„ç†
                print(f"   âœ… è´Ÿæ ·æœ¬æ•°é‡å·²ç»åˆç†ï¼Œæ— éœ€è°ƒæ•´")
                print(f"   å½“å‰è´Ÿæ ·æœ¬æ¯”ä¾‹: {current_ratio:.2f}:1 (ç›®æ ‡: {negative_ratio}:1)")
        
        # ä¿å­˜æ•°æ®åˆ—è¡¨
        import json
        list_file = output_path / 'data_list.json'
        with open(list_file, 'w') as f:
            json.dump(data_list, f, indent=2)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats['negative_ratio'] = negative_ratio if balance_samples else None
        stats['balanced'] = balance_samples
        stats_file = output_path / 'stats.json'
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"\nâœ… {split}é›†å‡†å¤‡å®Œæˆ!")
        print(f"   æ€»å›¾åƒæ•°: {stats['total_images']}")
        print(f"   æ€»å€™é€‰æ¡†æ•°: {stats['total_proposals']}")
        print(f"   æ­£æ ·æœ¬æ•°: {stats['positive_samples']}")
        print(f"   è´Ÿæ ·æœ¬æ•°: {stats['negative_samples']}")
        print(f"   æ­£è´Ÿæ¯”ä¾‹: 1:{stats['negative_samples']/max(stats['positive_samples'],1):.2f}")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {stats['class_dist']}")
        print(f"   æ•°æ®ä¿å­˜è‡³: {output_path}")


# ==================== åˆ†ç±»å™¨æ¨¡å‹ ====================

class SimpleMLP(nn.Module):
    """ç®€å•çš„MLPåˆ†ç±»å™¨"""
    
    def __init__(self, input_size: int = 112, num_classes: int = 2, 
                 hidden_dims: List[int] = [256, 128]):
        """
        Args:
            input_size: è¾“å…¥å›¾åƒå¤§å°
            num_classes: ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
            hidden_dims: éšè—å±‚ç»´åº¦
        """
        super().__init__()
        
        # ç®€å•çš„å·ç§¯ç‰¹å¾æå–å™¨
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            nn.AdaptiveAvgPool2d((4, 4))
        )
        
        # MLPåˆ†ç±»å¤´
        feature_dim = 128 * 4 * 4
        layers = []
        in_dim = feature_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.5)
            ])
            in_dim = hidden_dim
        
        layers.append(nn.Linear(in_dim, num_classes))
        
        self.classifier = nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


class MobileNetClassifier(nn.Module):
    """åŸºäºMobileNetV2çš„åˆ†ç±»å™¨"""
    
    def __init__(self, num_classes: int = 2, pretrained: bool = True, dropout: float = 0.5):
        """
        Args:
            num_classes: ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            dropout: Dropoutæ¯”ä¾‹ï¼ˆé»˜è®¤0.5ï¼‰
        """
        super().__init__()
        
        # æ£€æŸ¥æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹
        local_model_path = Path("pretrained_models/mobilenet_v2-b0353104.pth")
        
        if pretrained and local_model_path.exists():
            # ä»æœ¬åœ°åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            print(f"   ğŸ“¦ ä»æœ¬åœ°åŠ è½½MobileNetV2é¢„è®­ç»ƒæ¨¡å‹: {local_model_path}")
            self.backbone = models.mobilenet_v2(pretrained=False)
            state_dict = torch.load(local_model_path, map_location='cpu')
            self.backbone.load_state_dict(state_dict)
        else:
            # ä»ç½‘ç»œä¸‹è½½ï¼ˆéœ€è¦è”ç½‘ï¼‰
            if pretrained:
                print(f"   ğŸ“¥ ä»ç½‘ç»œä¸‹è½½MobileNetV2é¢„è®­ç»ƒæ¨¡å‹ï¼ˆéœ€è¦è”ç½‘ï¼‰...")
            self.backbone = models.mobilenet_v2(pretrained=pretrained)
        
        # æ›¿æ¢åˆ†ç±»å¤´ï¼ˆå¢åŠ Dropoutå’Œé¢å¤–çš„å…¨è¿æ¥å±‚ï¼‰
        in_features = self.backbone.classifier[1].in_features
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(p=dropout),
            nn.Linear(in_features, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        return self.backbone(x)


# ==================== Losså‡½æ•° ====================

class FocalLoss(nn.Module):
    """
    Focal Loss for Hard Example Mining
    
    è®ºæ–‡: Focal Loss for Dense Object Detection (https://arxiv.org/abs/1708.02002)
    
    ç”¨é€”ï¼šè‡ªåŠ¨å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬ï¼Œé™ä½ç®€å•æ ·æœ¬çš„æƒé‡
    
    Args:
        alpha: ç±»åˆ«æƒé‡ï¼Œç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
               - å¯ä»¥æ˜¯floatï¼ˆæ‰€æœ‰ç±»åˆ«ç»Ÿä¸€æƒé‡ï¼‰
               - å¯ä»¥æ˜¯listï¼ˆæ¯ä¸ªç±»åˆ«ä¸åŒæƒé‡ï¼‰
        gamma: focusing parameterï¼Œæ§åˆ¶éš¾æ˜“æ ·æœ¬çš„æƒé‡å·®å¼‚
               - gamma=0æ—¶é€€åŒ–ä¸ºæ ‡å‡†äº¤å‰ç†µ
               - gammaè¶Šå¤§ï¼Œç®€å•æ ·æœ¬æƒé‡è¶Šä½
               - æ¨èå€¼ï¼š2.0-5.0
    
    å…¬å¼ï¼š
        FL(p_t) = -Î±_t * (1 - p_t)^Î³ * log(p_t)
        
    å…¶ä¸­ p_t æ˜¯æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ï¼š
        - p_t æ¥è¿‘1ï¼ˆç®€å•æ ·æœ¬ï¼‰â†’ (1-p_t)^Î³ æ¥è¿‘0 â†’ losså¾ˆå°
        - p_t æ¥è¿‘0ï¼ˆéš¾æ ·æœ¬ï¼‰â†’ (1-p_t)^Î³ æ¥è¿‘1 â†’ lossæ­£å¸¸
    """
    
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        """
        åˆå§‹åŒ–Focal Loss
        
        Args:
            alpha: ç±»åˆ«æƒé‡ï¼Œé»˜è®¤0.25
            gamma: focusingå‚æ•°ï¼Œé»˜è®¤2.0
            reduction: 'mean' æˆ– 'sum'
        """
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        è®¡ç®—Focal Loss
        
        Args:
            inputs: [N, C] æ¨¡å‹è¾“å‡ºlogitsï¼ˆæœªç»è¿‡softmaxï¼‰
            targets: [N] ç±»åˆ«æ ‡ç­¾
        
        Returns:
            loss: scalar
        """
        # è®¡ç®—äº¤å‰ç†µlossï¼ˆä¸åšreductionï¼‰
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # è®¡ç®—é¢„æµ‹æ¦‚ç‡
        p = torch.exp(-ce_loss)  # p_t: æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡
        
        # è®¡ç®—focal weight
        focal_weight = (1 - p) ** self.gamma
        
        # è®¡ç®—focal loss
        focal_loss = self.alpha * focal_weight * ce_loss
        
        # Reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


# ==================== æ•°æ®é›† ====================

class CascadedDataset(Dataset):
    """çº§è”æ£€æµ‹æ•°æ®é›†"""
    
    def __init__(self, data_list_path: str, transform=None, num_classes: int = 2):
        """
        Args:
            data_list_path: æ•°æ®åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„
            transform: å›¾åƒå˜æ¢
            num_classes: ç±»åˆ«æ•°ï¼ˆä¸åŒ…æ‹¬èƒŒæ™¯ï¼‰
        """
        import json
        
        with open(data_list_path, 'r') as f:
            self.data_list = json.load(f)
        
        self.root_dir = Path(data_list_path).parent
        self.transform = transform
        self.num_classes = num_classes
    
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        item = self.data_list[idx]
        
        # è¯»å–å›¾åƒ
        img_path = self.root_dir / item['crop_path']
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # æ ‡ç­¾ï¼štrue_cls=-1è¡¨ç¤ºèƒŒæ™¯(label=0)ï¼Œå…¶ä»–ç±»åˆ«ä»1å¼€å§‹
        true_cls = item['true_cls']
        if true_cls == -1:
            label = 0  # èƒŒæ™¯
        else:
            label = true_cls + 1  # å‰æ™¯ç±»åˆ«ä»1å¼€å§‹
        
        return image, label


# ==================== è®­ç»ƒå™¨ ====================

class CascadedTrainer:
    """çº§è”æ£€æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, device: str = "cuda:0"):
        self.model = model.to(device)
        self.device = device
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader,
              num_epochs: int = 50, lr: float = 0.001, save_dir: str = "runs/cascaded_train",
              weight_decay: float = 0.01, patience: int = 10, 
              loss_type: str = 'focal', focal_alpha: float = 0.25, focal_gamma: float = 2.0):
        """
        è®­ç»ƒåˆ†ç±»å™¨
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            lr: å­¦ä¹ ç‡
            save_dir: ä¿å­˜ç›®å½•
            weight_decay: æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
            patience: æ—©åœçš„è€å¿ƒè½®æ•°
            loss_type: æŸå¤±å‡½æ•°ç±»å‹ ('ce' æˆ– 'focal')
            focal_alpha: Focal Lossçš„alphaå‚æ•°
            focal_gamma: Focal Lossçš„gammaå‚æ•°
        """
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨AdamWï¼Œå¸¦æƒé‡è¡°å‡ï¼‰
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        
        # é€‰æ‹©æŸå¤±å‡½æ•°
        if loss_type == 'focal':
            criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)
            print(f"   ä½¿ç”¨Focal Loss (alpha={focal_alpha}, gamma={focal_gamma})")
        else:
            criterion = nn.CrossEntropyLoss()
            print(f"   ä½¿ç”¨Cross Entropy Loss")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šå…ˆä½™å¼¦é€€ç«ï¼Œå†æ ¹æ®éªŒè¯é›†æ€§èƒ½è°ƒæ•´
        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
        scheduler_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3, verbose=True
        )
        
        best_val_acc = 0.0
        patience_counter = 0
        
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒåˆ†ç±»å™¨...")
        print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
        print(f"   éªŒè¯æ ·æœ¬æ•°: {len(val_loader.dataset)}")
        print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"   å­¦ä¹ ç‡: {lr}")
        print(f"   æƒé‡è¡°å‡: {weight_decay}")
        print(f"   æ—©åœè€å¿ƒ: {patience}è½®")
        print(f"   æŸå¤±å‡½æ•°: {loss_type}")
        
        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
            for images, labels in pbar:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                train_total += labels.size(0)
                train_correct += predicted.eq(labels).sum().item()
                
                pbar.set_postfix({
                    'loss': f"{train_loss/train_total:.4f}",
                    'acc': f"{100.*train_correct/train_total:.2f}%"
                })
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler_cosine.step()
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self.evaluate(val_loader, criterion)
            
            # æ ¹æ®éªŒè¯æ€§èƒ½è°ƒæ•´å­¦ä¹ ç‡
            scheduler_plateau.step(val_acc)
            
            print(f"\n   Epoch {epoch+1}: "
                  f"Train Loss={train_loss/len(train_loader):.4f}, "
                  f"Train Acc={100.*train_correct/train_total:.2f}%, "
                  f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ—©åœæ£€æŸ¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                torch.save(self.model.state_dict(), save_path / 'best.pt')
                print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc={val_acc:.2f}%)")
            else:
                patience_counter += 1
                print(f"   âš ï¸  éªŒè¯å‡†ç¡®ç‡æœªæå‡ ({patience_counter}/{patience})")
                
                if patience_counter >= patience:
                    print(f"\nğŸ›‘ æ—©åœ! {patience}è½®éªŒè¯å‡†ç¡®ç‡æœªæå‡")
                    break
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % 10 == 0:
                torch.save(self.model.state_dict(), save_path / f'epoch_{epoch+1}.pt')
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        print(f"   æ¨¡å‹ä¿å­˜è‡³: {save_path}")
    
    def evaluate(self, data_loader: DataLoader, criterion) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in data_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        avg_loss = total_loss / len(data_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy


# ==================== ä¸¤é˜¶æ®µæ¨ç†å™¨ ====================

class CascadedDetector:
    """ä¸¤é˜¶æ®µçº§è”æ£€æµ‹å™¨"""
    
    def __init__(self, yolo_model_path: str, classifier_path: str,
                 classifier_type: str = "mobilenet", input_size: int = 112,
                 num_classes: int = 2, conf_threshold: float = 0.05,
                 classifier_threshold: float = 0.5, device: str = "cuda:0",
                 cross_class_nms: bool = True, nms_iou: float = 0.3):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨
        
        Args:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            classifier_path: åˆ†ç±»å™¨æƒé‡è·¯å¾„
            classifier_type: åˆ†ç±»å™¨ç±»å‹ ('mlp' æˆ– 'mobilenet')
            input_size: åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸
            num_classes: ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
            conf_threshold: ç¬¬ä¸€é˜¶æ®µç½®ä¿¡åº¦é˜ˆå€¼
            classifier_threshold: ç¬¬äºŒé˜¶æ®µåˆ†ç±»é˜ˆå€¼
            device: è®¾å¤‡
            cross_class_nms: æ˜¯å¦ä½¿ç”¨è·¨ç±»åˆ«NMS
            nms_iou: è·¨ç±»åˆ«NMSçš„IOUé˜ˆå€¼
        """
        # åŠ è½½YOLOæ¨¡å‹
        self.yolo_model = YOLO(yolo_model_path)
        self.yolo_model.to(device)
        
        # åŠ è½½åˆ†ç±»å™¨
        if classifier_type == "mlp":
            self.classifier = SimpleMLP(input_size, num_classes)
        else:
            self.classifier = MobileNetClassifier(num_classes)
        
        self.classifier.load_state_dict(torch.load(classifier_path, map_location=device))
        self.classifier.to(device)
        self.classifier.eval()
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((input_size, input_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        self.conf_threshold = conf_threshold
        self.classifier_threshold = classifier_threshold
        self.device = device
        self.use_cross_class_nms = cross_class_nms
        self.nms_iou = nms_iou
        
        print(f"âœ… åŠ è½½ä¸¤é˜¶æ®µæ£€æµ‹å™¨")
        print(f"   YOLOæ¨¡å‹: {yolo_model_path}")
        print(f"   åˆ†ç±»å™¨: {classifier_path} (ç±»å‹: {classifier_type})")
        print(f"   è·¨ç±»åˆ«NMS: {'å¯ç”¨' if cross_class_nms else 'ç¦ç”¨'} (IOU={nms_iou})")
    
    def detect(self, image_path: str, imgsz: int = 1280) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µæ£€æµ‹
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            imgsz: YOLOæ¨ç†å°ºå¯¸
        
        Returns:
            æ£€æµ‹ç»“æœ [{'box': [x1,y1,x2,y2], 'cls': int, 'conf': float, 'stage1_cls': int, 'stage1_conf': float}, ...]
        """
        # è¯»å–å›¾åƒ
        img = cv2.imread(image_path)
        img_h, img_w = img.shape[:2]
        
        # ç¬¬ä¸€é˜¶æ®µï¼šYOLOç”Ÿæˆå€™é€‰æ¡†
        results = self.yolo_model.predict(
            source=image_path,
            imgsz=imgsz,
            conf=self.conf_threshold,
            iou=0.45,
            device=self.device,
            verbose=False,
            save=False,
        )
        
        result = results[0]
        detections = []
        
        if len(result.boxes) == 0:
            return detections
        
        xyxy = result.boxes.xyxy.cpu().numpy()
        cls = result.boxes.cls.cpu().numpy()
        conf = result.boxes.conf.cpu().numpy()
        
        # ç¬¬äºŒé˜¶æ®µï¼šåˆ†ç±»å™¨é‡åˆ†ç±»
        for i in range(len(xyxy)):
            x1, y1, x2, y2 = xyxy[i]
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            
            # è¾¹ç•Œæ£€æŸ¥
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(img_w, x2)
            y2 = min(img_h, y2)
            
            if x2 <= x1 or y2 <= y1:
                continue
            
            # è£å‰ªå€™é€‰åŒºåŸŸ
            crop = img[y1:y2, x1:x2]
            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
            
            # åˆ†ç±»å™¨æ¨ç†
            crop_tensor = self.transform(crop_pil).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                output = self.classifier(crop_tensor)
                probs = F.softmax(output, dim=1)
                stage2_conf, stage2_cls = probs.max(1)
                stage2_conf = stage2_conf.item()
                stage2_cls = stage2_cls.item()
            
            # è¿‡æ»¤ï¼šå¦‚æœåˆ†ç±»å™¨åˆ¤æ–­ä¸ºèƒŒæ™¯ï¼ˆcls=0ï¼‰æˆ–ç½®ä¿¡åº¦ä½ï¼Œåˆ™ä¸¢å¼ƒ
            if stage2_cls == 0 or stage2_conf < self.classifier_threshold:
                continue
            
            # è½¬æ¢ç±»åˆ«ï¼ˆåˆ†ç±»å™¨çš„ç±»åˆ«ä»1å¼€å§‹ï¼Œéœ€è¦å‡1ï¼‰
            final_cls = stage2_cls - 1
            
            detections.append({
                'box': [float(x1), float(y1), float(x2), float(y2)],
                'cls': final_cls,
                'conf': stage2_conf,
                'stage1_cls': int(cls[i]),
                'stage1_conf': float(conf[i])
            })
        
        # è·¨ç±»åˆ«NMSï¼ˆå¯é€‰ï¼‰
        if self.use_cross_class_nms and len(detections) > 0:
            detections = cross_class_nms(detections, self.nms_iou)
        
        return detections


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description="ä¸¤é˜¶æ®µçº§è”æ£€æµ‹ç³»ç»Ÿ")
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')
    
    # å‡†å¤‡æ•°æ®
    prepare_parser = subparsers.add_parser('prepare', help='å‡†å¤‡è®­ç»ƒæ•°æ®')
    prepare_parser.add_argument('--yolo-model', type=str, required=True, help='YOLOæ¨¡å‹è·¯å¾„')
    prepare_parser.add_argument('--data-yaml', type=str, default='/home/cjh/mmdetection/data/balloon/yolo_format/data.yaml', 
                               help='æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶')
    prepare_parser.add_argument('--conf', type=float, default=0.05, help='ç¬¬ä¸€é˜¶æ®µç½®ä¿¡åº¦é˜ˆå€¼')
    prepare_parser.add_argument('--iou', type=float, default=0.5, help='ä¸GTåŒ¹é…çš„IOUé˜ˆå€¼')
    prepare_parser.add_argument('--output-dir', type=str, default='cascaded_data', help='è¾“å‡ºç›®å½•')
    prepare_parser.add_argument('--imgsz', type=int, default=1280, help='æ¨ç†å°ºå¯¸')
    prepare_parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    prepare_parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ•°æ®ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰')
    prepare_parser.add_argument('--negative-ratio', type=float, default=2.0, 
                               help='è´Ÿæ ·æœ¬ä¸æ­£æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆé»˜è®¤2.0ï¼Œå³è´Ÿ:æ­£=2:1ï¼‰')
    prepare_parser.add_argument('--no-balance', action='store_true', 
                               help='ä¸è¿›è¡Œæ ·æœ¬å¹³è¡¡ï¼ˆä¿ç•™æ‰€æœ‰æ ·æœ¬ï¼‰')
    
    # è®­ç»ƒåˆ†ç±»å™¨
    train_parser = subparsers.add_parser('train', help='è®­ç»ƒåˆ†ç±»å™¨')
    train_parser.add_argument('--data-dir', type=str, required=True, help='æ•°æ®ç›®å½•')
    train_parser.add_argument('--model-type', type=str, default='mobilenet', 
                             choices=['mlp', 'mobilenet'], help='æ¨¡å‹ç±»å‹')
    train_parser.add_argument('--input-size', type=int, default=112, help='è¾“å…¥å›¾åƒå¤§å°')
    train_parser.add_argument('--num-classes', type=int, default=2, help='ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰')
    train_parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹å¤§å°')
    train_parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    train_parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    train_parser.add_argument('--save-dir', type=str, default='runs/cascaded_train', help='ä¿å­˜ç›®å½•')
    train_parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    
    # Focal Losså‚æ•°
    train_parser.add_argument('--loss-type', type=str, default='focal', 
                             choices=['ce', 'focal'], help='æŸå¤±å‡½æ•°ç±»å‹: ce (CrossEntropy) æˆ– focal (FocalLoss)')
    train_parser.add_argument('--focal-alpha', type=float, default=0.25,
                             help='Focal Lossçš„alphaå‚æ•°ï¼ˆç±»åˆ«æƒé‡ï¼‰')
    train_parser.add_argument('--focal-gamma', type=float, default=2.0,
                             help='Focal Lossçš„gammaå‚æ•°ï¼ˆéš¾æ˜“æ ·æœ¬æƒé‡å·®å¼‚ï¼Œæ¨è2.0-5.0ï¼‰')
    
    # æ¨ç†
    infer_parser = subparsers.add_parser('infer', help='ä¸¤é˜¶æ®µæ¨ç†')
    infer_parser.add_argument('--yolo-model', type=str, required=True, help='YOLOæ¨¡å‹è·¯å¾„')
    infer_parser.add_argument('--classifier', type=str, required=True, help='åˆ†ç±»å™¨æƒé‡è·¯å¾„')
    infer_parser.add_argument('--model-type', type=str, default='mobilenet',
                             choices=['mlp', 'mobilenet'], help='åˆ†ç±»å™¨ç±»å‹')
    infer_parser.add_argument('--image', type=str, required=True, help='è¾“å…¥å›¾åƒè·¯å¾„')
    infer_parser.add_argument('--imgsz', type=int, default=1280, help='YOLOæ¨ç†å°ºå¯¸')
    infer_parser.add_argument('--input-size', type=int, default=112, help='åˆ†ç±»å™¨è¾“å…¥å°ºå¯¸')
    infer_parser.add_argument('--num-classes', type=int, default=2, help='ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰')
    infer_parser.add_argument('--conf', type=float, default=0.05, help='ç¬¬ä¸€é˜¶æ®µç½®ä¿¡åº¦é˜ˆå€¼')
    infer_parser.add_argument('--cls-threshold', type=float, default=0.5, help='ç¬¬äºŒé˜¶æ®µåˆ†ç±»é˜ˆå€¼')
    infer_parser.add_argument('--cross-class-nms', action='store_true', default=True, help='å¯ç”¨è·¨ç±»åˆ«NMS')
    infer_parser.add_argument('--no-cross-class-nms', action='store_false', dest='cross_class_nms', help='ç¦ç”¨è·¨ç±»åˆ«NMS')
    infer_parser.add_argument('--nms-iou', type=float, default=0.3, help='è·¨ç±»åˆ«NMSçš„IOUé˜ˆå€¼')
    infer_parser.add_argument('--save-dir', type=str, default='runs/cascaded_infer', help='ä¿å­˜ç›®å½•')
    infer_parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    if args.command == 'prepare':
        # å‡†å¤‡æ•°æ®
        preparer = CascadedDataPreparer(
            args.yolo_model,
            conf_threshold=args.conf,
            iou_threshold=args.iou,
            device=args.device
        )
        balance_samples = not args.no_balance
        preparer.prepare_dataset(
            args.data_yaml, 'train', args.output_dir, args.imgsz, 
            args.force, args.negative_ratio, balance_samples
        )
        preparer.prepare_dataset(
            args.data_yaml, 'val', args.output_dir, args.imgsz, 
            args.force, args.negative_ratio, balance_samples
        )
        
    elif args.command == 'train':
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¢å¼ºçš„æ•°æ®å¢å¼ºï¼‰
        train_transform = transforms.Compose([
            transforms.Resize((args.input_size, args.input_size)),
            
            # å‡ ä½•å˜æ¢
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.5),
            transforms.RandomRotation(15),  # éšæœºæ—‹è½¬Â±15åº¦
            
            # é¢œè‰²å¢å¼º
            transforms.ColorJitter(
                brightness=0.2,  # äº®åº¦
                contrast=0.2,    # å¯¹æ¯”åº¦
                saturation=0.2,  # é¥±å’Œåº¦
                hue=0.1          # è‰²è°ƒ
            ),
            
            transforms.ToTensor(),
            
            # éšæœºæ“¦é™¤ï¼ˆæ¨¡æ‹Ÿé®æŒ¡ï¼‰
            transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),
            
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        val_transform = transforms.Compose([
            transforms.Resize((args.input_size, args.input_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        train_dataset = CascadedDataset(
            f"{args.data_dir}/train/data_list.json",
            transform=train_transform,
            num_classes=args.num_classes - 1  # å‡å»èƒŒæ™¯
        )
        
        val_dataset = CascadedDataset(
            f"{args.data_dir}/val/data_list.json",
            transform=val_transform,
            num_classes=args.num_classes - 1
        )
        
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, 
                                 shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, 
                               shuffle=False, num_workers=4)
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨æ›´é«˜çš„dropoutï¼‰
        if args.model_type == 'mlp':
            model = SimpleMLP(args.input_size, args.num_classes)
        else:
            model = MobileNetClassifier(args.num_classes, dropout=0.5)
        
        # è®­ç»ƒï¼ˆä½¿ç”¨æƒé‡è¡°å‡ã€æ—©åœå’ŒFocal Lossï¼‰
        trainer = CascadedTrainer(model, args.device)
        trainer.train(train_loader, val_loader, args.epochs, args.lr, args.save_dir,
                     weight_decay=0.01, patience=10,
                     loss_type=args.loss_type, focal_alpha=args.focal_alpha, focal_gamma=args.focal_gamma)
        
    elif args.command == 'infer':
        # ä¸¤é˜¶æ®µæ¨ç†
        detector = CascadedDetector(
            args.yolo_model,
            args.classifier,
            classifier_type=args.model_type,
            input_size=args.input_size,
            num_classes=args.num_classes,
            conf_threshold=args.conf,
            classifier_threshold=args.cls_threshold,
            device=args.device,
            cross_class_nms=args.cross_class_nms,
            nms_iou=args.nms_iou
        )
        
        detections = detector.detect(args.image, args.imgsz)
        
        print(f"\nâœ… æ£€æµ‹å®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
        for i, det in enumerate(detections):
            print(f"   {i+1}. ç±»åˆ«={det['cls']}, ç½®ä¿¡åº¦={det['conf']:.3f}, "
                  f"æ¡†={det['box']}, "
                  f"[Stage1: cls={det['stage1_cls']}, conf={det['stage1_conf']:.3f}]")
        
        # å¯è§†åŒ–
        img = cv2.imread(args.image)
        for det in detections:
            x1, y1, x2, y2 = [int(v) for v in det['box']]
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label = f"cls{det['cls']} {det['conf']:.2f}"
            cv2.putText(img, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 
                       0.5, (0, 255, 0), 2)
        
        # ä¿å­˜ç»“æœ
        save_path = Path(args.save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        output_file = save_path / f"{Path(args.image).stem}_cascaded.jpg"
        cv2.imwrite(str(output_file), img)
        print(f"   ç»“æœä¿å­˜è‡³: {output_file}")
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()

